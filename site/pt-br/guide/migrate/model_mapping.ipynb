{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bYaCABobL5q"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FlUw7tSKbtg4"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-fogOi3K7nR"
      },
      "source": [
        "# Use modelos TF1.x em workflows TF2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/migrate/model_mapping\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/migrate/model_mapping.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/migrate/model_mapping.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/migrate/model_mapping.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-GwECUqrkqT"
      },
      "source": [
        "Este guia fornece uma visão geral e exemplos de um [shim para código de modelagem](https://en.wikipedia.org/wiki/Shim_(computing)) que você pode empregar para usar seus modelos TF1.x existentes em workflows TF2, como estratégias de execução antecipada (eager), `tf.function` e de distribuição com alterações mínimas no seu código de modelagem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_ezCbogxaqt"
      },
      "source": [
        "## Escopo de uso\n",
        "\n",
        "O shim descrito neste guia foi projetado para modelos TF1.x que dependem de:\n",
        "\n",
        "1. `tf.compat.v1.get_variable` e `tf.compat.v1.variable_scope` para controlar a criação e reutilização de variáveis ​​e\n",
        "2. APIs baseadas em coleções de grafos, como `tf.compat.v1.global_variables()`, `tf.compat.v1.trainable_variables`, `tf.compat.v1.losses.get_regularization_losses()` e `tf.compat.v1.get_collection()` para acompanhar de pesos e perdas de regularização\n",
        "\n",
        "Isso inclui a maioria dos modelos criados com base nas APIs `tf.compat.v1.layer`, `tf.contrib.layers` e [TensorFlow-Slim](https://github.com/google-research/tf-slim).\n",
        "\n",
        "O shim **NÃO** é necessário para os seguintes modelos TF1.x:\n",
        "\n",
        "1. Modelos Keras standalone que já rastreiam todos os seus pesos treináveis ​​e perdas de regularização via `model.trainable_weights` e `model.losses`, respectivamente.\n",
        "2. `tf.Module`s que já rastreiam todos os seus pesos treináveis ​​via `module.trainable_variables` e apenas criam pesos se ainda não tiverem sido criados.\n",
        "\n",
        "Esses modelos provavelmente já funcionarão no TF2 com execução eager e `tf.function`s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OQNFp8zgV0C"
      },
      "source": [
        "## Configuração\n",
        "\n",
        "Importe o TensorFlow e outras dependências."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG2n3-qlD5mA"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y -q tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVfR3MBvD9Sc"
      },
      "outputs": [],
      "source": [
        "# Install tf-nightly as the DeterministicRandomTestTool is available only in\n",
        "# Tensorflow 2.8\n",
        "\n",
        "!pip install -q tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzkV-2cna823"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as v1\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from contextlib import contextmanager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox4kn0DK8H0f"
      },
      "source": [
        "## O decorador `track_tf1_style_variables`\n",
        "\n",
        "O principal shim descrito neste guia é `tf.compat.v1.keras.utils.track_tf1_style_variables`, um decorador que você pode usar nos métodos pertencentes a `tf.keras.layers.Layer` e `tf.Module` para rastrear pesos de estilo TF1.x e capturar perdas de regularização.\n",
        "\n",
        "Decorar métodos de chamada de objetos `tf.keras.layers.Layer` ou `tf.Module` com `tf.compat.v1.keras.utils.track_tf1_style_variables` permite a criação e reutilização de variáveis ​​via `tf.compat.v1.get_variable` (e por extensão `tf.compat.v1.layers`) para funcionar corretamente dentro do método decorado em vez de sempre criar uma nova variável a cada chamada. Isso também fará com que a camada ou módulo rastreie implicitamente quaisquer pesos criados ou acessados ​​via `get_variable` dentro do método decorado.\n",
        "\n",
        "Além de rastrear os próprios pesos sob as propriedades padrão `layer.variable`/`module.variable`/etc., se o método pertencer a um `tf.keras.layers.Layer`, quaisquer perdas de regularização especificadas através dos argumentos do regularizador `get_variable` ou `tf.compat.v1.layers` serão rastreadas pela camada sob a propriedade padrão `layer.losses`.\n",
        "\n",
        "Esse mecanismo de rastreamento permite o uso de grandes classes de código model-forward-pass no estilo TF1.x dentro das camadas Keras ou objetos `tf.Module` no TF2, mesmo com os comportamentos TF2 ativados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq6IqZILmGmO"
      },
      "source": [
        "## Exemplos de uso\n",
        "\n",
        "Os exemplos de uso abaixo demonstram os shims de modelagem usados ​​para decorar os métodos `tf.keras.layers.Layer`, mas, exceto onde eles interagem especificamente com os recursos do Keras, eles também são aplicáveis ​​ao decorar métodos `tf.Module`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWGPh6KmkHq6"
      },
      "source": [
        "### Camada construída com tf.compat.v1.get_variable\n",
        "\n",
        "Imagine que você tenha uma camada implementada diretamente sobre `tf.compat.v1.get_variable` da seguinte forma:\n",
        "\n",
        "```python\n",
        "def dense(self, inputs, units):\n",
        "  out = inputs\n",
        "  with tf.compat.v1.variable_scope(\"dense\"):\n",
        "    # The weights are created with a `regularizer`,\n",
        "    kernel = tf.compat.v1.get_variable(\n",
        "        shape=[out.shape[-1], units],\n",
        "        regularizer=tf.keras.regularizers.L2(),\n",
        "        initializer=tf.compat.v1.initializers.glorot_normal,\n",
        "        name=\"kernel\")\n",
        "    bias = tf.compat.v1.get_variable(\n",
        "        shape=[units,],\n",
        "        initializer=tf.compat.v1.initializers.zeros,\n",
        "        name=\"bias\")\n",
        "    out = tf.linalg.matmul(out, kernel)\n",
        "    out = tf.compat.v1.nn.bias_add(out, bias)\n",
        "  return out\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sZWU7JSok2n"
      },
      "source": [
        "Use o shim para transformá-la numa camada e chame-a nas entradas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3eKkcKtS_N4"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    out = inputs\n",
        "    with tf.compat.v1.variable_scope(\"dense\"):\n",
        "      # The weights are created with a `regularizer`,\n",
        "      # so the layer should track their regularization losses\n",
        "      kernel = tf.compat.v1.get_variable(\n",
        "          shape=[out.shape[-1], self.units],\n",
        "          regularizer=tf.keras.regularizers.L2(),\n",
        "          initializer=tf.compat.v1.initializers.glorot_normal,\n",
        "          name=\"kernel\")\n",
        "      bias = tf.compat.v1.get_variable(\n",
        "          shape=[self.units,],\n",
        "          initializer=tf.compat.v1.initializers.zeros,\n",
        "          name=\"bias\")\n",
        "      out = tf.linalg.matmul(out, kernel)\n",
        "      out = tf.compat.v1.nn.bias_add(out, bias)\n",
        "    return out\n",
        "\n",
        "layer = DenseLayer(10)\n",
        "x = tf.random.normal(shape=(8, 20))\n",
        "layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqXAlWnYgwcq"
      },
      "source": [
        "Acesse as variáveis ​​rastreadas e as perdas de regularização capturadas como uma camada Keras padrão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNz5HmkXg0B5"
      },
      "outputs": [],
      "source": [
        "layer.trainable_variables\n",
        "layer.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0z9GmRlhM9X"
      },
      "source": [
        "Para ver se os pesos são reutilizados sempre que você chama a camada, defina todos os pesos como zero e chame a camada novamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ4vOu2Rf-I2"
      },
      "outputs": [],
      "source": [
        "print(\"Resetting variables to zero:\", [var.name for var in layer.trainable_variables])\n",
        "\n",
        "for var in layer.trainable_variables:\n",
        "  var.assign(var * 0.0)\n",
        "\n",
        "# Note: layer.losses is not a live view and\n",
        "# will get reset only at each layer call\n",
        "print(\"layer.losses:\", layer.losses)\n",
        "print(\"calling layer again.\")\n",
        "out = layer(x)\n",
        "print(\"layer.losses: \", layer.losses)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwEprtA-lOh6"
      },
      "source": [
        "Você também pode usar a camada convertida diretamente na construção do modelo funcional Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E7ZCINHlaHU"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(20))\n",
        "outputs = DenseLayer(10)(inputs)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "x = tf.random.normal(shape=(8, 20))\n",
        "model(x)\n",
        "\n",
        "# Access the model variables and regularization losses\n",
        "model.weights\n",
        "model.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew5TTEyZkZGU"
      },
      "source": [
        "### Modelo criado com `tf.compat.v1.layers`\n",
        "\n",
        "Imagine que você tenha uma camada ou modelo implementado diretamente sobre `tf.compat.v1.layers` da seguinte forma:\n",
        "\n",
        "```python\n",
        "def model(self, inputs, units):\n",
        "  with tf.compat.v1.variable_scope('model'):\n",
        "    out = tf.compat.v1.layers.conv2d(\n",
        "        inputs, 3, 3,\n",
        "        kernel_regularizer=\"l2\")\n",
        "    out = tf.compat.v1.layers.flatten(out)\n",
        "    out = tf.compat.v1.layers.dense(\n",
        "        out, units,\n",
        "        kernel_regularizer=\"l2\")\n",
        "    return out\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZolXllfpVx6"
      },
      "source": [
        "Use o shim para transformá-lo numa camada e chame-a nas entradas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBpfSHWTTTCv"
      },
      "outputs": [],
      "source": [
        "class CompatV1LayerModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          inputs, 3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      out = tf.compat.v1.layers.flatten(out)\n",
        "      out = tf.compat.v1.layers.dense(\n",
        "          out, self.units,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n",
        "\n",
        "layer = CompatV1LayerModel(10)\n",
        "x = tf.random.normal(shape=(8, 5, 5, 5))\n",
        "layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkG9oLlblfK_"
      },
      "source": [
        "Aviso: por questões de segurança, certifique-se de colocar todos os `tf.compat.v1.layers` dentro de uma string não vazia `variable_scope`. O motivo é que `tf.compat.v1.layers` com nomes gerados automaticamente sempre incrementará automaticamente o nome quando estiver fora de qualquer escopo de variável. Isso significa que os nomes das variáveis ​​solicitadas serão incompatíveis sempre que você chamar a camada/módulo. Assim, em vez de reutilizar os pesos já criados, ele criará um novo conjunto de variáveis ​​a cada chamada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAVN6dy3p7ik"
      },
      "source": [
        "Acesse as variáveis ​​rastreadas e as perdas de regularização capturadas como uma camada Keras padrão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTRF99vJp7ik"
      },
      "outputs": [],
      "source": [
        "layer.trainable_variables\n",
        "layer.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkNuEcyIp7ik"
      },
      "source": [
        "Para ver se os pesos são reutilizados sempre que você chama a camada, defina todos os pesos como zero e chame a camada novamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dk4XScdp7il"
      },
      "outputs": [],
      "source": [
        "print(\"Resetting variables to zero:\", [var.name for var in layer.trainable_variables])\n",
        "\n",
        "for var in layer.trainable_variables:\n",
        "  var.assign(var * 0.0)\n",
        "\n",
        "out = layer(x)\n",
        "print(\"layer.losses: \", layer.losses)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zD3a8PKzU7S"
      },
      "source": [
        "Você também pode usar a camada convertida diretamente na construção do modelo funcional Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q88BgBCup7il"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(5, 5, 5))\n",
        "outputs = CompatV1LayerModel(10)(inputs)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "x = tf.random.normal(shape=(8, 5, 5, 5))\n",
        "model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cioB6Zap7il"
      },
      "outputs": [],
      "source": [
        "# Access the model variables and regularization losses\n",
        "model.weights\n",
        "model.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBNODOx9ly6r"
      },
      "source": [
        "### Capture atualizações de normalização em lote e argumentos de modelo `training`\n",
        "\n",
        "No TF1.x, você executa a normalização em lote da seguinte forma:\n",
        "\n",
        "```python\n",
        "  x_norm = tf.compat.v1.layers.batch_normalization(x, training=training)\n",
        "\n",
        "  # ...\n",
        "\n",
        "  update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "  train_op = optimizer.minimize(loss)\n",
        "  train_op = tf.group([train_op, update_ops])\n",
        "```\n",
        "\n",
        "Observe que:\n",
        "\n",
        "1. As atualizações de média móvel da normalização em lote são rastreadas por `get_collection`, que foi chamado separadamente da camada\n",
        "2. `tf.compat.v1.layers.batch_normalization` requer um argumento `training` (geralmente chamado de `is_training` ao usar camadas de normalização em lote TF-Slim)\n",
        "\n",
        "No TF2, devido à [execução antecipada](https://www.tensorflow.org/guide/eager) (eager) e às dependências de controle automático, as atualizações da média móvel da normalização do lote serão executadas imediatamente. Não há necessidade de coletá-las separadamente da coleção de atualizações e adicioná-las como dependências de controle explícitas.\n",
        "\n",
        "Além disso, se você fornecer um argumento `training` ao método de passo para frente da sua `tf.keras.layers.Layer`, o Keras poderá passar a fase de treinamento atual e quaisquer camadas aninhadas para ele, assim como faria com qualquer outra camada. Consulte a documentação da API para `tf.keras.Model` para obter mais informações sobre como o Keras lida com o argumento `training`.\n",
        "\n",
        "Se você estiver decorando métodos `tf.Module`, precisará certificar-se de passar manualmente todos os argumentos `training` conforme necessário. No entanto, as atualizações de média móvel de normalização de lote ainda serão aplicadas automaticamente sem a necessidade de dependências de controle explícitas.\n",
        "\n",
        "As amostras de código a seguir demonstram como incorporar camadas de normalização de lote no shim e como funciona seu uso num modelo Keras (aplicável a `tf.keras.layers.Layer`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjZE-J7mkS9p"
      },
      "outputs": [],
      "source": [
        "class CompatV1BatchNorm(tf.keras.layers.Layer):\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    print(\"Forward pass called with `training` =\", training)\n",
        "    with v1.variable_scope('batch_norm_layer'):\n",
        "      return v1.layers.batch_normalization(x, training=training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGuvvElmY-fu"
      },
      "outputs": [],
      "source": [
        "print(\"Constructing model\")\n",
        "inputs = tf.keras.Input(shape=(5, 5, 5))\n",
        "outputs = CompatV1BatchNorm()(inputs)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "print(\"Calling model in inference mode\")\n",
        "x = tf.random.normal(shape=(8, 5, 5, 5))\n",
        "model(x, training=False)\n",
        "\n",
        "print(\"Moving average variables before training: \",\n",
        "      {var.name: var.read_value() for var in model.non_trainable_variables})\n",
        "\n",
        "# Notice that when running TF2 and eager execution, the batchnorm layer directly\n",
        "# updates the moving averages while training without needing any extra control\n",
        "# dependencies\n",
        "print(\"calling model in training mode\")\n",
        "model(x, training=True)\n",
        "\n",
        "print(\"Moving average variables after training: \",\n",
        "      {var.name: var.read_value() for var in model.non_trainable_variables})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gai4ikpmeRqR"
      },
      "source": [
        "### Reuso variável baseado em escopo de variáveis\n",
        "\n",
        "Quaisquer criações de variáveis ​​no passo para frente baseado em `get_variable` manterão a mesma nomenclatura de variável e semântica de reutilização que os escopos de variáveis ​​têm no TF1.x. Isto se aplica desde que você tenha pelo menos um escopo externo não vazio para quaisquer `tf.compat.v1.layers` com nomes gerados automaticamente, conforme mencionado acima.\n",
        "\n",
        "Observação: A nomenclatura e a reutilização terão como escopo uma única instância de camada/módulo. As chamadas para `get_variable` dentro de uma camada ou módulo decorado com shim não poderão se referir a variáveis ​​criadas dentro de camadas ou módulos. Você pode contornar isso usando referências Python a outras variáveis ​​diretamente, se necessário, em vez de acessar variáveis ​​via `get_variable`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PzYZdX2nMVt"
      },
      "source": [
        "### Execução avançada (eager) e `tf.function`\n",
        "\n",
        "Como visto acima, os métodos decorados para `tf.keras.layers.Layer` e `tf.Module` são executados dentro da execução antecipada (eager) e também são compatíveis com `tf.function`. Isto significa que você pode usar [pdb](https://docs.python.org/3/library/pdb.html) e outras ferramentas interativas para percorrer seu passo para frente durante a execução.\n",
        "\n",
        "Importante: Embora seja perfeitamente seguro chamar seus métodos de camada/módulo decorados com shim de *dentro* de um `tf.function`, não é seguro colocar `tf.functions` dentro de seus métodos decorados com shim se esses `tf.functions` contiverem chamadas `get_variable`. Inserir um `tf.function` redefine os `variable_scope`, o que significa que o reuso de uma variável (baseada em escopo de variável no estilo TF1.x que o shim simula) será quebrada nessa configuração."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPytVgZWnShe"
      },
      "source": [
        "### Estratégias de distribuição\n",
        "\n",
        "Chamadas para `get_variable` dentro de `@track_tf1_style_variables` - camada decorada ou métodos de módulo usam criações padrão da variável `tf.Variable` nos bastidores. Isto significa que você pode usá-las com as várias estratégias de distribuição disponíveis com `tf.distribute`, como `MirroredStrategy` e `TPUStrategy`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DcK24FOA8A2"
      },
      "source": [
        "## Aninhando objetos `tf.Variable`, `tf.Module`, `tf.keras.layers` e `tf.keras.models` em chamadas decoradas\n",
        "\n",
        "Decorar sua chamada de camada em `tf.compat.v1.keras.utils.track_tf1_style_variables` vai apenas adicionar rastreamento implícito automático das variáveis ​​criadas (e reutilizadas) via `tf.compat.v1.get_variable`. Isto não vai capturar pesos criados diretamente por chamadas `tf.Variable`, como aquelas usadas por camadas típicas do Keras e a maioria dos objetos `tf.Module`. Esta seção descreve como lidar com esses casos aninhados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azxza3bVOZlv"
      },
      "source": [
        "### (Usos pré-existentes) `tf.keras.layers` e `tf.keras.models`\n",
        "\n",
        "Para usos pré-existentes de camadas e modelos Keras aninhados, use `tf.compat.v1.keras.utils.get_or_create_layer`. Isto é recomendado apenas para facilitar a migração dos usos de Keras aninhados em TF1.x existentes; o novo código deve usar configuração de atributo explícito conforme descrito abaixo para tf.Variables e tf.Modules.\n",
        "\n",
        "Para usar `tf.compat.v1.keras.utils.get_or_create_layer`, envolva o código que constrói seu modelo aninhado dentro de um método e passe-o para o método. Por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN15TcRgHKsq"
      },
      "outputs": [],
      "source": [
        "class NestedModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  def build_model(self):\n",
        "    inp = tf.keras.Input(shape=(5, 5))\n",
        "    dense_layer = tf.keras.layers.Dense(\n",
        "        10, name=\"dense\", kernel_regularizer=\"l2\",\n",
        "        kernel_initializer=tf.compat.v1.ones_initializer())\n",
        "    model = tf.keras.Model(inputs=inp, outputs=dense_layer(inp))\n",
        "    return model\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    # Get or create a nested model without assigning it as an explicit property\n",
        "    model = tf.compat.v1.keras.utils.get_or_create_layer(\n",
        "        \"dense_model\", self.build_model)\n",
        "    return model(inputs)\n",
        "\n",
        "layer = NestedModel(10)\n",
        "layer(tf.ones(shape=(5,5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgsKlltPHI8z"
      },
      "source": [
        "Este método garante que essas camadas aninhadas sejam reutilizadas e rastreadas corretamente pelo Tensorflow. Observe que o decorador `@track_tf1_style_variables` ainda é necessário no método apropriado. O método para construção do modelo passado para `get_or_create_layer` (neste caso, `self.build_model`), não deve receber argumentos.\n",
        "\n",
        "Os pesos são rastreados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zO5A78MJsqO"
      },
      "outputs": [],
      "source": [
        "assert len(layer.weights) == 2\n",
        "weights = {x.name: x for x in layer.variables}\n",
        "\n",
        "assert set(weights.keys()) == {\"dense/bias:0\", \"dense/kernel:0\"}\n",
        "\n",
        "layer.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Xsi-JbKTuj"
      },
      "source": [
        "E a perda de regularização também:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdK5RGm5KW5C"
      },
      "outputs": [],
      "source": [
        "tf.add_n(layer.losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_VRycQYJrXu"
      },
      "source": [
        "### Migração incremental: `tf.Variables` e `tf.Modules`\n",
        "\n",
        "Se você precisar incorporar chamadas `tf.Variable` ou `tf.Module` em seus métodos decorados (por exemplo, se estiver seguindo a migração incremental para APIs TF2 não legadas descritas mais adiante neste guia), você ainda precisará rastreá-las explicitamente, com os seguintes requisitos:\n",
        "\n",
        "- Garanta, de forma explicita, que a variável/módulo/camada seja criada apenas uma vez\n",
        "- Anexe-as, de forma explícita, como atributos de instância, assim como você faria ao definir um [módulo ou camada típico](https://www.tensorflow.org/guide/intro_to_modules#defining_models_and_layers_in_tensorflow)\n",
        "- Reutilize, de forma explícita, o objeto já criado em chamadas subsequentes\n",
        "\n",
        "Isso garante que os pesos não sejam criados a cada nova chamada e sejam reutilizados corretamente. Além disso, isso também garante que os pesos existentes e as perdas de regularização sejam rastreados.\n",
        "\n",
        "Eis um exemplo de como isso pode ser feito:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrRPPoJ5ap5U"
      },
      "outputs": [],
      "source": [
        "class NestedLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def __call__(self, inputs):\n",
        "    out = inputs\n",
        "    with tf.compat.v1.variable_scope(\"inner_dense\"):\n",
        "      # The weights are created with a `regularizer`,\n",
        "      # so the layer should track their regularization losses\n",
        "      kernel = tf.compat.v1.get_variable(\n",
        "          shape=[out.shape[-1], self.units],\n",
        "          regularizer=tf.keras.regularizers.L2(),\n",
        "          initializer=tf.compat.v1.initializers.glorot_normal,\n",
        "          name=\"kernel\")\n",
        "      bias = tf.compat.v1.get_variable(\n",
        "          shape=[self.units,],\n",
        "          initializer=tf.compat.v1.initializers.zeros,\n",
        "          name=\"bias\")\n",
        "      out = tf.linalg.matmul(out, kernel)\n",
        "      out = tf.compat.v1.nn.bias_add(out, bias)\n",
        "    return out\n",
        "\n",
        "class WrappedDenseLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.units = units\n",
        "    # Only create the nested tf.variable/module/layer/model\n",
        "    # once, and then reuse it each time!\n",
        "    self._dense_layer = NestedLayer(self.units)\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    with tf.compat.v1.variable_scope('outer'):\n",
        "      outputs = tf.compat.v1.layers.dense(inputs, 3)\n",
        "      outputs = tf.compat.v1.layers.dense(inputs, 4)\n",
        "      return self._dense_layer(outputs)\n",
        "\n",
        "layer = WrappedDenseLayer(10)\n",
        "\n",
        "layer(tf.ones(shape=(5, 5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo9h6wc6bmEF"
      },
      "source": [
        "Observe que o rastreamento explícito do módulo aninhado é necessário, mesmo que seja decorado com o decorador `track_tf1_style_variables`. Isso ocorre porque cada módulo/camada com métodos decorados possui seu próprio armazenamento de variáveis ​​associado a ele.\n",
        "\n",
        "Os pesos são rastreados corretamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt6USaTVbauM"
      },
      "outputs": [],
      "source": [
        "assert len(layer.weights) == 6\n",
        "weights = {x.name: x for x in layer.variables}\n",
        "\n",
        "assert set(weights.keys()) == {\"outer/inner_dense/bias:0\",\n",
        "                               \"outer/inner_dense/kernel:0\",\n",
        "                               \"outer/dense/bias:0\",\n",
        "                               \"outer/dense/kernel:0\",\n",
        "                               \"outer/dense_1/bias:0\",\n",
        "                               \"outer/dense_1/kernel:0\"}\n",
        "\n",
        "layer.trainable_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHn-bJoNJw7l"
      },
      "source": [
        "Assim como a perda de regularização:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq5GFtXjJyut"
      },
      "outputs": [],
      "source": [
        "layer.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7VKJj3JOCEk"
      },
      "source": [
        "Observe que, se `NestedLayer` fosse um `tf.Module` não Keras, as variáveis ​​ainda seriam rastreadas, mas as perdas de regularização não seriam rastreadas automaticamente, então você teria que rastreá-las explicitamente de forma separada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsTgnydkdezQ"
      },
      "source": [
        "### Orientação sobre nomes de variáveis\n",
        "\n",
        "Chamadas explícitas `tf.Variable` e camadas Keras usam um mecanismo de geração automática de nome de variável / nome de camada diferente do que você pode estar acostumado a partir da combinação de `get_variable` e `variable_scopes`. Embora o shim faça com que os nomes de variáveis ​​correspondam às variáveis ​​criadas por `get_variable`, mesmo ao migrar de grafos TF1.x para TF2 com execução antecipada (eager) e `tf.function`, ele não poderá garantir o mesmo para os nomes de variáveis ​​gerados para chamadas `tf.Variable` e camadas Keras que você incorpora dentro de seus decoradores de método. É até possível ter múltiplas variáveis ​​compartilhando o mesmo nome na execução antecipada (eager) TF2 e `tf.function`.\n",
        "\n",
        "Você deve dedicar uma atenção especial a essa questão quando for seguir as seções sobre validação da exatidão e mapeamento de checkpoints TF1.x mais adiante neste guia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaP7fxoUWfMm"
      },
      "source": [
        "### Usando `tf.compat.v1.make_template` no método decorado\n",
        "\n",
        "**É altamente recomendável que você use `tf.compat.v1.keras.utils.track_tf1_style_variables` diretamente, em vez de usar `tf.compat.v1.make_template`, pois é uma camada mais fina sobre TF2**.\n",
        "\n",
        "Siga as orientações desta seção para código TF1.x existente que já dependia de `tf.compat.v1.make_template`.\n",
        "\n",
        "Como `tf.compat.v1.make_template` agrupa o código que usa `get_variable`, o decorador `track_tf1_style_variables` permite que você use esses modelos em chamadas de camada e rastreie com sucesso os pesos e as perdas de regularização.\n",
        "\n",
        "No entanto, não deixe de chamar `make_template` apenas uma vez e depois reutilizar o mesmo modelo em cada chamada de camada. Caso contrário, um novo modelo será criado cada vez que você chamar a camada junto com um novo conjunto de variáveis.\n",
        "\n",
        "Por exemplo,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHEQN8z44dbK"
      },
      "outputs": [],
      "source": [
        "class CompatV1TemplateScaleByY(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    def my_op(x, scalar_name):\n",
        "      var1 = tf.compat.v1.get_variable(scalar_name,\n",
        "                            shape=[],\n",
        "                            regularizer=tf.compat.v1.keras.regularizers.L2(),\n",
        "                            initializer=tf.compat.v1.constant_initializer(1.5))\n",
        "      return x * var1\n",
        "    self.scale_by_y = tf.compat.v1.make_template('scale_by_y', my_op, scalar_name='y')\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    with tf.compat.v1.variable_scope('layer'):\n",
        "      # Using a scope ensures the `scale_by_y` name will not be incremented\n",
        "      # for each instantiation of the layer.\n",
        "      return self.scale_by_y(inputs)\n",
        "\n",
        "layer = CompatV1TemplateScaleByY()\n",
        "\n",
        "out = layer(tf.ones(shape=(2, 3)))\n",
        "print(\"weights:\", layer.weights)\n",
        "print(\"regularization loss:\", layer.losses)\n",
        "print(\"output:\", out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKTJ7IsTEe8"
      },
      "source": [
        "Importante: Evite compartilhar o mesmo modelo criado por `make_template` em múltiplas instâncias de camada, pois isto poderá quebrar os mecanismos de rastreamento de perda de regularização e variável do decorador shim. Além disso, se você planeja usar o mesmo nome `make_template` dentro de múltiplas instâncias de camada, precisará aninhar o uso do modelo criado dentro de um `variable_scope`. Caso contrário, o nome gerado para a `variable_scope` do modelo será incrementado a cada nova instância da camada. Isto poderá alterar os nomes dos pesos de maneiras inesperadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4E3-XPhWD2N"
      },
      "source": [
        "## Migração incremental para TF2 nativo\n",
        "\n",
        "Conforme mencionado anteriormente, `track_tf1_style_variables` permite que você misture `tf.Variable`/`tf.keras.layers.Layer`/`tf.Module` orientado a objetos no estilo TF2 com o uso legado de `tf.compat.v1.get_variable`/`tf.compat.v1.layers` dentro do mesmo módulo/camada decorada.\n",
        "\n",
        "Isto significa que depois de tornar seu modelo TF1.x totalmente compatível com TF2, você pode escrever todos os novos componentes de modelo com APIs TF2 nativas (não- `tf.compat.v1`) e fazer com que interoperem com seu código antigo.\n",
        "\n",
        "No entanto, se você continuar a modificar seus componentes de modelo mais antigos, você talvez também queira gradualmente substituir o uso do `tf.compat.v1` legado para as APIs orientadas a objeto puramente nativas recomendadas para código TF2 novo.\n",
        "\n",
        "O uso de `tf.compat.v1.get_variable` pode ser substituído por chamadas `self.add_weight` se você estiver decorando uma camada/modelo Keras ou com chamadas `tf.Variable` se estiver decorando objetos Keras ou `tf.Module`.\n",
        "\n",
        "Tanto as `tf.compat.v1.layers` de estilo funcional como as orientadas a objeto geralmente podem ser substituídas por uma camada `tf.keras.layers` equivalente sem a necessidade de alterações nos argumentos.\n",
        "\n",
        "Você talvez também queira considerar partes de seu modelo ou padrões comuns em camadas/módulos individuais durante sua mudança incremental para APIs puramente nativas, que podem usar `track_tf1_style_variables`.\n",
        "\n",
        "### Uma observação sobre Slim e contrib.layers\n",
        "\n",
        "Bastante código TF 1.x antigo usa a biblioteca [Slim](https://ai.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html), que foi empacotada com o TF 1.x em `tf.contrib.layers`. Converter código que usa Slim para TF 2 nativo é mais complexo do que converter `v1.layers`. Na verdade, pode ser melhor converter seu código Slim para `v1.layers` primeiro e depois converter para Keras. Abaixo estão algumas orientações gerais para converter o código Slim.\n",
        "\n",
        "- Garanta que todos os argumentos sejam explícitos. Remova `arg_scopes` se possível. Se você ainda precisar usá-los, separe `normalizer_fn` e `activation_fn` em camadas próprias.\n",
        "- Camadas convolucionais separáveis ​​são mapeadas para uma ou mais camadas Keras diferentes (camadas Keras separáveis, ponto a ponto, profundidade a profundidade).\n",
        "- Slim e `v1.layers` têm nomes de argumentos e valores padrão diferentes.\n",
        "- Observe que alguns argumentos usam escalas diferentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFoULo-gazit"
      },
      "source": [
        "### Migração para TF2 nativo ignorando a compatibilidade de checkpoints\n",
        "\n",
        "O exemplo de código a seguir demonstra a migração incremental de um modelo para APIs puramente nativas sem considerar a compatibilidade dos checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPO9YJsb6r-D"
      },
      "outputs": [],
      "source": [
        "class CompatModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          inputs, 3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      out = tf.compat.v1.layers.flatten(out)\n",
        "      out = tf.compat.v1.layers.dropout(out, training=training)\n",
        "      out = tf.compat.v1.layers.dense(\n",
        "          out, self.units,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp16xK6Oa8k9"
      },
      "source": [
        "Em seguida, substitua as APIs `compat.v1` por suas equivalentes nativas orientadas a objetos, de forma segmentada. Comece trocando a camada de convolução por um objeto Keras criado no construtor da camada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOj1Swe16so3"
      },
      "outputs": [],
      "source": [
        "class PartiallyMigratedModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "    self.conv_layer = tf.keras.layers.Conv2D(\n",
        "      3, 3,\n",
        "      kernel_regularizer=\"l2\")\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_layer(inputs)\n",
        "      out = tf.compat.v1.layers.flatten(out)\n",
        "      out = tf.compat.v1.layers.dropout(out, training=training)\n",
        "      out = tf.compat.v1.layers.dense(\n",
        "          out, self.units,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzJF0H0sbce8"
      },
      "source": [
        "Use a classe [`v1.keras.utils.DeterministicRandomTestTool`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/utils/DeterministicRandomTestTool) para verificar se essa alteração incremental deixa o modelo com o mesmo comportamento de antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTJq0qW9_Tz2"
      },
      "outputs": [],
      "source": [
        "random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')\n",
        "with random_tool.scope():\n",
        "  tf.keras.utils.set_random_seed(42)\n",
        "  layer = CompatModel(10)\n",
        "\n",
        "  inputs = tf.random.normal(shape=(10, 5, 5, 5))\n",
        "  original_output = layer(inputs)\n",
        "\n",
        "  # Grab the regularization loss as well\n",
        "  original_regularization_loss = tf.math.add_n(layer.losses)\n",
        "\n",
        "print(original_regularization_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4Wq3wuaHjEV"
      },
      "outputs": [],
      "source": [
        "random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')\n",
        "with random_tool.scope():\n",
        "  tf.keras.utils.set_random_seed(42)\n",
        "  layer = PartiallyMigratedModel(10)\n",
        "\n",
        "  inputs = tf.random.normal(shape=(10, 5, 5, 5))\n",
        "  migrated_output = layer(inputs)\n",
        "\n",
        "  # Grab the regularization loss as well\n",
        "  migrated_regularization_loss = tf.math.add_n(layer.losses)\n",
        "\n",
        "print(migrated_regularization_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMMXS7EHjvCy"
      },
      "outputs": [],
      "source": [
        "# Verify that the regularization loss and output both match\n",
        "np.testing.assert_allclose(original_regularization_loss.numpy(), migrated_regularization_loss.numpy())\n",
        "np.testing.assert_allclose(original_output.numpy(), migrated_output.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMxiMVFwbiQy"
      },
      "source": [
        "Agora você substituiu todos os `compat.v1.layers` individuais por camadas Keras nativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dFCnyYc9DrX"
      },
      "outputs": [],
      "source": [
        "class NearlyFullyNativeModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "    self.conv_layer = tf.keras.layers.Conv2D(\n",
        "      3, 3,\n",
        "      kernel_regularizer=\"l2\")\n",
        "    self.flatten_layer = tf.keras.layers.Flatten()\n",
        "    self.dense_layer = tf.keras.layers.Dense(\n",
        "      self.units,\n",
        "      kernel_regularizer=\"l2\")\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_layer(inputs)\n",
        "      out = self.flatten_layer(out)\n",
        "      out = self.dense_layer(out)\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGPqEjkGHgar"
      },
      "outputs": [],
      "source": [
        "random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')\n",
        "with random_tool.scope():\n",
        "  tf.keras.utils.set_random_seed(42)\n",
        "  layer = NearlyFullyNativeModel(10)\n",
        "\n",
        "  inputs = tf.random.normal(shape=(10, 5, 5, 5))\n",
        "  migrated_output = layer(inputs)\n",
        "\n",
        "  # Grab the regularization loss as well\n",
        "  migrated_regularization_loss = tf.math.add_n(layer.losses)\n",
        "\n",
        "print(migrated_regularization_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAs60eCdj6x_"
      },
      "outputs": [],
      "source": [
        "# Verify that the regularization loss and output both match\n",
        "np.testing.assert_allclose(original_regularization_loss.numpy(), migrated_regularization_loss.numpy())\n",
        "np.testing.assert_allclose(original_output.numpy(), migrated_output.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA6viSo3bo3y"
      },
      "source": [
        "Por fim, remova qualquer uso restante de `variable_scope` (não é mais necessário) e o decorador `track_tf1_style_variables`.\n",
        "\n",
        "Agora você tem uma versão do modelo que usa APIs totalmente nativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIHpHWIRDunU"
      },
      "outputs": [],
      "source": [
        "class FullyNativeModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.units = units\n",
        "    self.conv_layer = tf.keras.layers.Conv2D(\n",
        "      3, 3,\n",
        "      kernel_regularizer=\"l2\")\n",
        "    self.flatten_layer = tf.keras.layers.Flatten()\n",
        "    self.dense_layer = tf.keras.layers.Dense(\n",
        "      self.units,\n",
        "      kernel_regularizer=\"l2\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    out = self.conv_layer(inputs)\n",
        "    out = self.flatten_layer(out)\n",
        "    out = self.dense_layer(out)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttAmiCvLHW54"
      },
      "outputs": [],
      "source": [
        "random_tool = v1.keras.utils.DeterministicRandomTestTool(mode='num_random_ops')\n",
        "with random_tool.scope():\n",
        "  tf.keras.utils.set_random_seed(42)\n",
        "  layer = FullyNativeModel(10)\n",
        "\n",
        "  inputs = tf.random.normal(shape=(10, 5, 5, 5))\n",
        "  migrated_output = layer(inputs)\n",
        "\n",
        "  # Grab the regularization loss as well\n",
        "  migrated_regularization_loss = tf.math.add_n(layer.losses)\n",
        "\n",
        "print(migrated_regularization_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym5DYtT4j7e3"
      },
      "outputs": [],
      "source": [
        "# Verify that the regularization loss and output both match\n",
        "np.testing.assert_allclose(original_regularization_loss.numpy(), migrated_regularization_loss.numpy())\n",
        "np.testing.assert_allclose(original_output.numpy(), migrated_output.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX4pdrzycIsa"
      },
      "source": [
        "### Mantendo a compatibilidade de checkpoints durante a migração para o Native TF2\n",
        "\n",
        "O processo de migração acima para APIs TF2 nativas alterou os nomes das variáveis ​​(já que as APIs Keras produzem nomes de pesos muito diferentes) e os caminhos orientados a objetos que apontam para pesos diferentes no modelo. O impacto dessas alterações é que elas quebrarão qualquer checkpoint baseado em nome no estilo TF1 existente ou checkpoint orientado a objeto no estilo TF2.\n",
        "\n",
        "No entanto, em alguns casos, você pode pegar seu checkpoint original, baseado em nome, e encontrar um mapeamento das variáveis ​​para seus novos nomes com abordagens como a detalhada no [Guia Reuso de checkpoints TF1.x](./migrating_checkpoints.ipynb).\n",
        "\n",
        "Algumas dicas para tornar isso viável são as seguintes:\n",
        "\n",
        "- As variáveis ​​ainda têm um argumento `name` que você pode definir.\n",
        "- Os modelos Keras também recebem um argumento `name` que eles definem como prefixo para suas variáveis.\n",
        "- A função `v1.name_scope` pode ser usada para definir prefixos de nomes de variáveis. Isto é muito diferente de `tf.variable_scope`. Afeta apenas nomes e não rastreia variáveis ​​e reuso.\n",
        "\n",
        "Levando em conta os ponteiros acima, os exemplos de código a seguir demonstram um fluxo de trabalho que você pode adaptar ao seu código para atualizar de forma incremental parte de um modelo enquanto atualiza simultaneamente os checkpoints.\n",
        "\n",
        "Observação: devido à complexidade da nomenclatura de variáveis ​​com camadas Keras, não há garantia de que isso funcione em todos os casos de uso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFmMY3dcx3mR"
      },
      "source": [
        "1. Comece trocando as `tf.compat.v1.layers` de estilo funcional por suas versões orientadas a objetos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRxCFmNjl2ta"
      },
      "outputs": [],
      "source": [
        "class FunctionalStyleCompatModel(tf.keras.layers.Layer):\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          inputs, 3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          out, 4, 4,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          out, 5, 5,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n",
        "\n",
        "layer = FunctionalStyleCompatModel()\n",
        "layer(tf.ones(shape=(10, 10, 10, 10)))\n",
        "[v.name for v in layer.weights]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvzUyXxjydAd"
      },
      "source": [
        "1. Em seguida, atribua os objetos compat.v1.layer e quaisquer variáveis ​​criadas por `compat.v1.get_variable` como propriedades do objeto `tf.keras.layers.Layer`/`tf.Module` cujo método é decorado com `track_tf1_style_variables` (observe que agora, quaisquer checkpoints orientados a objeto em estilo TF2 irão salvar dois caminhos: um por nome de variável e o novo caminho orientado a objetos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02jMQkJFmFwl"
      },
      "outputs": [],
      "source": [
        "class OOStyleCompatModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.conv_1 = tf.compat.v1.layers.Conv2D(\n",
        "          3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "    self.conv_2 = tf.compat.v1.layers.Conv2D(\n",
        "          4, 4,\n",
        "          kernel_regularizer=\"l2\")\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_1(inputs)\n",
        "      out = self.conv_2(out)\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          out, 5, 5,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n",
        "\n",
        "layer = OOStyleCompatModel()\n",
        "layer(tf.ones(shape=(10, 10, 10, 10)))\n",
        "[v.name for v in layer.weights]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8evFpd8Nq63v"
      },
      "source": [
        "1. Salve novamente um checkpoint carregado neste ponto para salvar os caminhos pelo nome da variável (para compat.v1.layers) ou pelo grafo orientado a objetos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7neFr-9pqmJX"
      },
      "outputs": [],
      "source": [
        "weights = {v.name: v for v in layer.weights}\n",
        "assert weights['model/conv2d/kernel:0'] is layer.conv_1.kernel\n",
        "assert weights['model/conv2d_1/bias:0'] is layer.conv_2.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvsi743Xh9wn"
      },
      "source": [
        "1. Agora você pode trocar os `compat.v1.layers` orientados a objetos por camadas Keras nativas enquanto ainda poderá carregar o checkpoint salvo recentemente. Certifique-se de preservar os nomes das variáveis ​​para as `compat.v1.layers` restantes, registrando os `variable_scopes` gerados automaticamente das camadas substituídas. Agora, essas camadas/variáveis ​​trocadas usarão apenas o caminho do atributo do objeto para as variáveis ​​no checkpoint, em vez do caminho do nome da variável.\n",
        "\n",
        "Em geral, você pode substituir o uso de `compat.v1.get_variable` em variáveis ​​anexadas a propriedades através de:\n",
        "\n",
        "- Trocando-as pelo uso de `tf.Variable`, **OU**\n",
        "- Atualizando-as com [`tf.keras.layers.Layer.add_weight`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight). Observe que, se você não estiver trocando todas as camadas de uma só vez, poderá haver troca nos nomes das camadas/variáveis geradas automaticamente para as `compat.v1.layers` restantes que não tiverem um argumento `name`. Se for esse o caso, você deve manter os nomes das variáveis ​​para as `compat.v1.layers` restantes abrindo e fechando manualmente um `variable_scope` correspondente ao nome do escopo gerado pela `compat.v1.layer` removida. Caso contrário, os caminhos dos checkpoints existentes podem entrar em conflito e o carregamento do checkpoint ocorrerá de forma incorreta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbixtIW-maoH"
      },
      "outputs": [],
      "source": [
        "def record_scope(scope_name):\n",
        "  \"\"\"Record a variable_scope to make sure future ones get incremented.\"\"\"\n",
        "  with tf.compat.v1.variable_scope(scope_name):\n",
        "    pass\n",
        "\n",
        "class PartiallyNativeKerasLayersModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.conv_1 = tf.keras.layers.Conv2D(\n",
        "          3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "    self.conv_2 = tf.keras.layers.Conv2D(\n",
        "          4, 4,\n",
        "          kernel_regularizer=\"l2\")\n",
        "\n",
        "  @tf.compat.v1.keras.utils.track_tf1_style_variables\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_1(inputs)\n",
        "      record_scope('conv2d') # Only needed if follow-on compat.v1.layers do not pass a `name` arg\n",
        "      out = self.conv_2(out)\n",
        "      record_scope('conv2d_1') # Only needed if follow-on compat.v1.layers do not pass a `name` arg\n",
        "      out = tf.compat.v1.layers.conv2d(\n",
        "          out, 5, 5,\n",
        "          kernel_regularizer=\"l2\")\n",
        "      return out\n",
        "\n",
        "layer = PartiallyNativeKerasLayersModel()\n",
        "layer(tf.ones(shape=(10, 10, 10, 10)))\n",
        "[v.name for v in layer.weights]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eaPpevGs3dA"
      },
      "source": [
        "Salvar um checkpoint nesta etapa depois da construção das variáveis ​​fará com que ele contenha ***apenas*** os caminhos de objeto atualmente disponíveis.\n",
        "\n",
        "Não deixe de registrar os escopos das `compat.v1.layers` removidas para preservar os nomes de peso gerados automaticamente para as `compat.v1.layers` restantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK7vtWBprObA"
      },
      "outputs": [],
      "source": [
        "weights = set(v.name for v in layer.weights)\n",
        "assert 'model/conv2d_2/kernel:0' in weights\n",
        "assert 'model/conv2d_2/bias:0' in weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ5-SfmWFTvY"
      },
      "source": [
        "1. Repita os passos acima até ter substituído todos os `compat.v1.layers` e `compat.v1.get_variable` do seu modelo por equivalentes totalmente nativos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA1d2POtnTQa"
      },
      "outputs": [],
      "source": [
        "class FullyNativeKerasLayersModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.conv_1 = tf.keras.layers.Conv2D(\n",
        "          3, 3,\n",
        "          kernel_regularizer=\"l2\")\n",
        "    self.conv_2 = tf.keras.layers.Conv2D(\n",
        "          4, 4,\n",
        "          kernel_regularizer=\"l2\")\n",
        "    self.conv_3 = tf.keras.layers.Conv2D(\n",
        "          5, 5,\n",
        "          kernel_regularizer=\"l2\")\n",
        "\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    with tf.compat.v1.variable_scope('model'):\n",
        "      out = self.conv_1(inputs)\n",
        "      out = self.conv_2(out)\n",
        "      out = self.conv_3(out)\n",
        "      return out\n",
        "\n",
        "layer = FullyNativeKerasLayersModel()\n",
        "layer(tf.ones(shape=(10, 10, 10, 10)))\n",
        "[v.name for v in layer.weights]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZejG7rTsTb6"
      },
      "source": [
        "Lembre-se de testar para garantir que o checkpoint recém-atualizado ainda se comporte conforme o esperado. Aplique as técnicas descritas no [guia de validação da exatidão numérica](./validate_correctness.ipynb) em cada passo incremental deste processo para garantir que seu código migrado seja executado corretamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewi_h-cs6n-I"
      },
      "source": [
        "## Lidando com mudanças de comportamento na migração de TF1.x para TF2 não cobertas pelos shims de modelagem\n",
        "\n",
        "Os shims de modelagem descritos neste guia podem garantir que variáveis, camadas e perdas de regularização criadas com `get_variable`, `tf.compat.v1.layers` e semânticas `variable_scope` continuem a funcionar como antes ao usar execução antecipada (eager) e `tf.function`, sem precisar depender de coleções.\n",
        "\n",
        "Isto não abrange ***todas as*** semânticas específicas do TF1.x das quais podem depender os passos para frente do seu modelo. Em alguns casos, os shims podem ser insuficientes para garantir que o passo para frente do seu modelo rode no TF2 por conta própria. Leia o [Guia de comportamentos TF1.x vs TF2](./tf1_vs_tf2) para saber mais sobre as diferenças comportamentais entre TF1.x e TF2."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "model_mapping.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
