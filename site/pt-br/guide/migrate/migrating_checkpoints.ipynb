{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bYaCABobL5q"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FlUw7tSKbtg4"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61dp4Hg5ksTC"
      },
      "source": [
        "# Como migrar os checkpoints de um modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/migrate/migrating_checkpoints\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/migrate/migrating_checkpoints.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/migrate/migrating_checkpoints.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/migrate/migrating_checkpoints.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avuMwzscPnHh"
      },
      "source": [
        "Observação: geralmente, chamamos os checkpoints salvos com `tf.compat.v1.Saver` de checkpoints *TF1 ou baseados em nome*. Chamamos os checkpoints salvos com `tf.train.Checkpoint` como checkpoints *TF2 ou baseados em objetos*.\n",
        "\n",
        "## Visão geral\n",
        "\n",
        "Este guia pressupõe que você tenha um modelo que salva e carrega checkpoints com [`tf.compat.v1.Saver`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver) e deseja migrar o código usando a API TF2 [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) ou usar checkpoints pré-existentes em seu modelo TF2.\n",
        "\n",
        "Abaixo estão alguns cenários comuns que você pode encontrar:\n",
        "\n",
        "**Cenário 1**\n",
        "\n",
        "Existem checkpoints TF1 existentes de execuções de treinamento anteriores que precisam ser carregados ou convertidos para TF2.\n",
        "\n",
        "- Para carregar o checkpoint TF1 em TF2, consulte a amostra de código [*Carregando um checkpoint TF1 em TF2*](#load-tf1-in-tf2).\n",
        "- Para converter o checkpoint em TF2, veja [*Conversão de checkpoints*](#checkpoint-conversion).\n",
        "\n",
        "**Cenário 2**\n",
        "\n",
        "Você está ajustando seu modelo de uma maneira tal que existe o risco de alterar nomes e caminhos de variáveis ​​(como na migração incremental de `get_variable` para a criação explícita de `tf.Variable`) e gostaria de continuar salvando/carregando checkpoints existentes ao longo do caminho.\n",
        "\n",
        "Consulte a seção sobre [*Como manter a compatibilidade de checkpoints durante a migração de modelos*](#maintain-checkpoint-compat)\n",
        "\n",
        "**Cenário 3**\n",
        "\n",
        "Você está migrando seu código de treinamento e checkpoints para TF2, mas seu pipeline de inferência continua exigindo checkpoints TF1 por enquanto (para estabilidade da produção).\n",
        "\n",
        "*Opção 1*\n",
        "\n",
        "Salve ambos os checkpoints TF1 e TF2 ao treinar.\n",
        "\n",
        "- veja [*Salvando um checkpoint TF1 em TF2*](#save-tf1-in-tf2)\n",
        "\n",
        "*Opção 2*\n",
        "\n",
        "Converta o checkpoint TF2 para TF1.\n",
        "\n",
        "- veja [*Conversão de checkpoints*](#checkpoint-conversion)\n",
        "\n",
        "---\n",
        "\n",
        "Os exemplos abaixo mostram todas as combinações de salvamento e carregamento de checkpoints no TF1/TF2, para que você tenha alguma flexibilidade para determinar como migrar seu modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaYgaekzOAHf"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcvTd5QhZ78L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "\n",
        "def print_checkpoint(save_path):\n",
        "  reader = tf.train.load_checkpoint(save_path)\n",
        "  shapes = reader.get_variable_to_shape_map()\n",
        "  dtypes = reader.get_variable_to_dtype_map()\n",
        "  print(f\"Checkpoint at '{save_path}':\")\n",
        "  for key in shapes:\n",
        "    print(f\"  (key='{key}', shape={shapes[key]}, dtype={dtypes[key].name}, \"\n",
        "          f\"value={reader.get_tensor(key)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO8Q6QkulJlj"
      },
      "source": [
        "## Mudanças de TF1 para TF2\n",
        "\n",
        "Esta seção está incluída se você tiver curiosidade sobre o que mudou entre TF1 e TF2 e o que queremos dizer com checkpoints \"baseados em nome\" (TF1) versus \"baseados em objeto\" (TF2).\n",
        "\n",
        "Na verdade, os dois tipos de checkpoint são salvos no mesmo formato, que é essencialmente uma tabela chave-valor. A diferença está em como as chaves são geradas.\n",
        "\n",
        "As chaves em checkpoints baseados em nome são os **nomes das variáveis**. As chaves em checkpoints baseados em objeto referem-se ao **caminho do objeto raiz para a variável** (os exemplos abaixo ajudarão a entender melhor o que isso significa).\n",
        "\n",
        "Primeiro, salve alguns checkpoints:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YXzbXvOWvdF"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    saver = tf1.train.Saver()\n",
        "    sess.run(a.assign(1))\n",
        "    sess.run(b.assign(2))\n",
        "    sess.run(c.assign(3))\n",
        "    saver.save(sess, 'tf1-ckpt')\n",
        "\n",
        "print_checkpoint('tf1-ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raOych1UaJzl"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(5.0, name='a')\n",
        "b = tf.Variable(6.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(7.0, name='c')\n",
        "\n",
        "ckpt = tf.train.Checkpoint(variables=[a, b, c])\n",
        "save_path_v2 = ckpt.save('tf2-ckpt')\n",
        "print_checkpoint(save_path_v2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYyLhTYszcpl"
      },
      "source": [
        "Se você observar as chaves em `tf2-ckpt`, todas elas se referem aos caminhos de objeto de cada variável. Por exemplo, a variável `a` é o primeiro elemento na lista `variables`, então sua chave se torna `variables/0/...` (sinta-se à vontade para ignorar a constante .ATTRIBUTES/VARIABLE_VALUE).\n",
        "\n",
        "Uma inspeção mais detalhada do objeto `Checkpoint` é mostrada abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLOxvoosg4Al"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "c = tf.Variable(0.)\n",
        "root = ckpt = tf.train.Checkpoint(variables=[a, b, c])\n",
        "print(\"root type =\", type(root).__name__)\n",
        "print(\"root.variables =\", root.variables)\n",
        "print(\"root.variables[0] =\", root.variables[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qaed1yAm3Ar"
      },
      "source": [
        "Experimente o trecho abaixo e veja como as chaves do checkpoint mudam com a estrutura do objeto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdHJXlZOyDnn"
      },
      "outputs": [],
      "source": [
        "module = tf.Module()\n",
        "module.d = tf.Variable(0.)\n",
        "test_ckpt = tf.train.Checkpoint(v={'a': a, 'b': b}, \n",
        "                                c=c,\n",
        "                                module=module)\n",
        "test_ckpt_path = test_ckpt.save('root-tf2-ckpt')\n",
        "print_checkpoint(test_ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iWitEsayDWs"
      },
      "source": [
        "*Por que o TF2 usa esse mecanismo?*\n",
        "\n",
        "Como não há mais um grafo global no TF2, os nomes das variáveis ​​não são confiáveis ​​e podem se tornar inconsistentes entre programas. O TF2 incentiva a abordagem de modelagem orientada a objetos onde as variáveis ​​pertencem às camadas, e as camadas pertencem a um modelo:\n",
        "\n",
        "```\n",
        "variable = tf.Variable(...)\n",
        "layer.variable_name = variable\n",
        "model.layer_name = layer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kv9SmyVjGLA"
      },
      "source": [
        "## Como manter a compatibilidade de checkpoints durante a migração de modelos\n",
        "\n",
        "<a name=\"maintain-checkpoint-compat\"></a>\n",
        "\n",
        "Uma etapa importante no processo de migração é *garantir que todas as variáveis ​​sejam inicializadas com os valores corretos*, o que, por sua vez, permitirá validar se as operações/funções estão fazendo os cálculos corretos. Para isto, deve-se considerar a **compatibilidade de checkpoints** entre modelos nas diversas etapas da migração. Essencialmente, esta seção responde à pergunta: *como continuo usando o mesmo checkpoint enquanto altero o modelo*.\n",
        "\n",
        "Abaixo estão três maneiras de manter a compatibilidade do checkpoint, a fim de aumentar a flexibilidade:\n",
        "\n",
        "1. O modelo tem os **mesmos nomes de variáveis** ​​de antes.\n",
        "2. O modelo tem nomes de variáveis ​​diferentes e mantém um **mapa de atribuições** que mapeia nomes de variáveis ​​no checkpoint para os novos nomes.\n",
        "3. O modelo tem nomes de variáveis ​​diferentes e mantém um **objeto TF2 Checkpoint** que armazena todas as variáveis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5JhCyPZDx43"
      },
      "source": [
        "### Quando os nomes das variáveis ​​correspondem\n",
        "\n",
        "Título longo: Como reutilizar os checkpoints quando os nomes das variáveis ​​coincidem.\n",
        "\n",
        "Resposta curta: você pode carregar diretamente o checkpoint pré-existente com `tf1.train.Saver` ou `tf.train.Checkpoint`.\n",
        "\n",
        "---\n",
        "\n",
        "Se você estiver usando `tf.compat.v1.keras.utils.track_tf1_style_variables`, então isto irá garantir que os nomes das variáveis ​​do modelo sejam os mesmos de antes. Você também pode garantir manualmente que os nomes das variáveis ​​correspondam.\n",
        "\n",
        "Quando os nomes das variáveis ​​correspondem nos modelos migrados, você pode usar `tf.train.Checkpoint` diretamente ou `tf.compat.v1.train.Saver` para carregar o checkpoint. Ambas as APIs são compatíveis com o modo grafo e eager, para que você possa usá-las em qualquer estágio da migração.\n",
        "\n",
        "Observação: você pode usar `tf.train.Checkpoint` para carregar checkpoints TF1, mas não pode usar `tf.compat.v1.Saver` para carregar checkpoints TF2 sem uma correspondência de nome complexa.\n",
        "\n",
        "Abaixo estão alguns exemplos de como usar o mesmo checkpoint com modelos diferentes. Primeiro, salve um checkpoint TF1 com `tf1.train.Saver`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijlHS96URsfR"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    saver = tf1.train.Saver()\n",
        "    sess.run(a.assign(1))\n",
        "    sess.run(b.assign(2))\n",
        "    sess.run(c.assign(3))\n",
        "    save_path = saver.save(sess, 'tf1-ckpt')\n",
        "print_checkpoint(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg7nWZphQD9u"
      },
      "source": [
        "O exemplo abaixo usa `tf.compat.v1.Saver` para carregar o checkpoint no modo eager:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4K16m0PPncQ"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.0, name='a')\n",
        "b = tf.Variable(0.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(0.0, name='c')\n",
        "\n",
        "# With the removal of collections in TF2, you must pass in the list of variables\n",
        "# to the Saver object:\n",
        "saver = tf1.train.Saver(var_list=[a, b, c])\n",
        "saver.restore(sess=None, save_path=save_path)\n",
        "print(f\"loaded values of [a, b, c]:  [{a.numpy()}, {b.numpy()}, {c.numpy()}]\")\n",
        "\n",
        "# Saving also works in eager (sess must be None).\n",
        "path = saver.save(sess=None, save_path='tf1-ckpt-saved-in-eager')\n",
        "print_checkpoint(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWnq1f5yAPkq"
      },
      "source": [
        "O próximo trecho de código carrega o checkpoint usando a API TF2 `tf.train.Checkpoint`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StyrzwGvW1YZ"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.0, name='a')\n",
        "b = tf.Variable(0.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(0.0, name='c')\n",
        "\n",
        "# Without the name_scope, name=\"scoped/c\" works too:\n",
        "c_2 = tf.Variable(0.0, name='scoped/c')\n",
        "\n",
        "print(\"Variable names: \")\n",
        "print(f\"  a.name = {a.name}\")\n",
        "print(f\"  b.name = {b.name}\")\n",
        "print(f\"  c.name = {c.name}\")\n",
        "print(f\"  c_2.name = {c_2.name}\")\n",
        "\n",
        "# Restore the values with tf.train.Checkpoint\n",
        "ckpt = tf.train.Checkpoint(variables=[a, b, c, c_2])\n",
        "ckpt.restore(save_path)\n",
        "print(f\"loaded values of [a, b, c, c_2]:  [{a.numpy()}, {b.numpy()}, {c.numpy()}, {c_2.numpy()}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYYgbj8F7Yb7"
      },
      "source": [
        "#### Nomes de variáveis ​​em TF2\n",
        "\n",
        "- As variáveis ​​ainda têm um argumento `name` que você pode definir.\n",
        "- Os modelos de Keras também recebem um argumento `name` que é definido como prefixo para suas variáveis.\n",
        "- A função `v1.name_scope` pode ser usada para definir prefixos de nomes de variáveis. Isso é muito diferente de `tf.variable_scope`. Ele afeta apenas nomes e não rastreia variáveis ​​e reutilização.\n",
        "\n",
        "O decorador `tf.compat.v1.keras.utils.track_tf1_style_variables` é um shim que ajuda a manter os nomes de variáveis ​​e a compatibilidade do checkpoint TF1, mantendo a nomenclatura e a semântica de reutilização de `tf.variable_scope` e `tf.compat.v1.get_variable` inalterada. Consulte o [Guia de mapeamento de modelos](./model_mapping.ipynb) para mais informações.\n",
        "\n",
        "**Observação 1: Se você estiver usando o shim, use APIs TF2 para carregar seus checkpoints (mesmo quando usar checkpoints TF1 pré-treinados).**\n",
        "\n",
        "Veja a seção *Checkpoints no Keras*.\n",
        "\n",
        "**Observação 2: ao migrar de `get_variable` para `tf.Variable`:**\n",
        "\n",
        "Se sua camada ou módulo decorado com shim consistir em algumas variáveis ​​(ou camadas/modelos Keras) que usam `tf.Variable` em vez de `tf.compat.v1.get_variable` e forem anexadas como propriedades/rastreadas de maneira orientada a objetos, elas podem ter diferentes semântica de nomenclatura variável em grafos/sessões TF1.x em comparação com a execução antecipada (eager).\n",
        "\n",
        "Em suma, *os nomes podem não ser o que você espera* ao executar no TF2.\n",
        "\n",
        "Importante: as variáveis ​​podem ter nomes duplicados na execução antecipada (eager), o que pode causar problemas se múltiplas variáveis ​​no checkpoint baseado em nome precisarem ser mapeadas para o mesmo nome. Você pode ajustar explicitamente a camada e os nomes das variáveis ​​usando `tf.name_scope` e o construtor da camada, ou argumentos `tf.Variable` `name` para ajustar os nomes das variáveis ​​e garantir que não haja duplicatas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkUQJUUyjOJz"
      },
      "source": [
        "### Manutenção de mapas de atribuições\n",
        "\n",
        "Os mapas de atribuição são frequentemente usados ​​para transferir pesos entre modelos TF1 e também podem ser usados ​​durante a migração do modelo se os nomes das variáveis ​​forem alterados.\n",
        "\n",
        "Você pode usar esses mapas com [`tf.compat.v1.train.init_from_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/init_from_checkpoint), [`tf.compat.v1.train.Saver`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver) e [`tf.train.load_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/load_checkpoint) para carregar pesos em modelos nos quais os nomes de variável ou escopo podem ter mudado.\n",
        "\n",
        "Os exemplos nesta seção usarão um <code>name</code> salvo anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PItyo7DdJ6Ek"
      },
      "outputs": [],
      "source": [
        "print_checkpoint('tf1-ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPryV_WBJrI3"
      },
      "source": [
        "#### Carregando com `init_from_checkpoint`\n",
        "\n",
        "[`tf1.train.init_from_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/init_from_checkpoint) precisa ser chamado durante um Grafo/Sessão, porque coloca os valores nos inicializadores de variáveis ​​em vez de criar um op de atribuição.\n",
        "\n",
        "Você pode usar o argumento `assignment_map` para configurar como as variáveis ​​são carregadas. Da documentação:\n",
        "\n",
        "> O mapa de atribuição suporta a seguinte sintaxe:\n",
        "\n",
        "- `'checkpoint_scope_name/': 'scope_name/'` - carregará todas as variáveis ​​no `scope_name` atual de `checkpoint_scope_name` com nomes de tensor correspondentes.\n",
        "- `'checkpoint_scope_name/some_other_variable': 'scope_name/variable_name'` - inicializará a variável `scope_name/variable_name` de `checkpoint_scope_name/some_other_variable`.\n",
        "- `'scope_variable_name': variable` - inicializará determinado objeto `tf.Variable` com o tensor 'scope_variable_name' do checkpoint.\n",
        "- `'scope_variable_name': list(variable)` - inicializará a lista de variáveis ​​particionadas com o tensor 'scope_variable_name' do checkpoint.\n",
        "- `'/': 'scope_name/'` - carregará todas as variáveis ​​no `scope_name` atual da raiz do checkpoint (por exemplo: 'no scope').\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM_7OzRpdH0A"
      },
      "outputs": [],
      "source": [
        "# Restoring with tf1.train.init_from_checkpoint:\n",
        "\n",
        "# A new model with a different scope for the variables.\n",
        "with tf.Graph().as_default() as g:\n",
        "  with tf1.variable_scope('new_scope'):\n",
        "    a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    # The assignment map will remap all variables in the checkpoint to the\n",
        "    # new scope:\n",
        "    tf1.train.init_from_checkpoint(\n",
        "        'tf1-ckpt',\n",
        "        assignment_map={'/': 'new_scope/'})\n",
        "    # `init_from_checkpoint` adds the initializers to these variables.\n",
        "    # Use `sess.run` to run these initializers.\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za_8xhFWKVlH"
      },
      "source": [
        "#### Carregando com `tf1.train.Saver`\n",
        "\n",
        "Ao contrário de `init_from_checkpoint`, [`tf.compat.v1.train.Saver`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver) é executado tanto no modo grafo quanto no modo eager. O argumento `var_list` opcionalmente aceita um dicionário, no entanto precisa mapear nomes de variáveis ​​para o objeto `tf.Variable`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiKNmdGJgoX9"
      },
      "outputs": [],
      "source": [
        "# Restoring with tf1.train.Saver (works in both graph and eager):\n",
        "\n",
        "# A new model with a different scope for the variables.\n",
        "with tf1.variable_scope('new_scope'):\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                      initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                      initializer=tf1.zeros_initializer())\n",
        "  c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "# Initialize the saver with a dictionary with the original variable names:\n",
        "saver = tf1.train.Saver({'a': a, 'b': b, 'scoped/c': c})\n",
        "saver.restore(sess=None, save_path='tf1-ckpt')\n",
        "print(\"Restored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JsgCXt3Ly-h"
      },
      "source": [
        "#### Carregando com `tf.train.load_checkpoint`\n",
        "\n",
        "Esta opção é ideal se você precisar de controle preciso sobre os valores das variáveis. Novamente, isto funciona nos modos grafo e eager."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc39Bh6JMso6"
      },
      "outputs": [],
      "source": [
        "# Restoring with tf.train.load_checkpoint (works in both graph and eager):\n",
        "\n",
        "# A new model with a different scope for the variables.\n",
        "with tf.Graph().as_default() as g:\n",
        "  with tf1.variable_scope('new_scope'):\n",
        "    a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "    c = tf1.get_variable('scoped/c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    # It may be easier writing a loop if your model has a lot of variables.\n",
        "    reader = tf.train.load_checkpoint('tf1-ckpt')\n",
        "    sess.run(a.assign(reader.get_tensor('a')))\n",
        "    sess.run(b.assign(reader.get_tensor('b')))\n",
        "    sess.run(c.assign(reader.get_tensor('scoped/c')))\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBSTJVCNDKed"
      },
      "source": [
        "### Mantendo um objeto TF2 Checkpoint\n",
        "\n",
        "Se os nomes de variável e escopo puderem mudar bastante durante a migração, use os checkpoints `tf.train.Checkpoint` e TF2. TF2 usa a **estrutura de objeto** em vez de nomes de variáveis ​​(mais detalhes em *Mudanças de TF1 para TF2*).\n",
        "\n",
        "Em suma, ao criar um `tf.train.Checkpoint` para salvar ou restaurar checkpoints, garanta que ele use o mesmo **ordenamento** (para listas) e **chaves** (para dicionários e argumentos de palavra-chave para o inicializador `Checkpoint`). Eis alguns exemplos de compatibilidade de checkpoint:\n",
        "\n",
        "```\n",
        "ckpt = tf.train.Checkpoint(foo=[var_a, var_b])\n",
        "\n",
        "# compatible with ckpt\n",
        "tf.train.Checkpoint(foo=[var_a, var_b])\n",
        "\n",
        "# not compatible with ckpt\n",
        "tf.train.Checkpoint(foo=[var_b, var_a])\n",
        "tf.train.Checkpoint(bar=[var_a, var_b])\n",
        "```\n",
        "\n",
        "Os exemplos de código abaixo mostram como usar o \"mesmo\" `tf.train.Checkpoint` para carregar variáveis ​​com nomes diferentes. Primeiro, salve um checkpoint do TF2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCSkz_-Tbct6"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(1))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(2))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(3))\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    print(\"[a, b, c]: \", sess.run([a, b, c]))\n",
        "\n",
        "    # Save a TF2 checkpoint\n",
        "    ckpt = tf.train.Checkpoint(unscoped=[a, b], scoped=[c])\n",
        "    tf2_ckpt_path = ckpt.save('tf2-ckpt')\n",
        "    print_checkpoint(tf2_ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62MWdZMxezeP"
      },
      "source": [
        "Você pode continuar usando `tf.train.Checkpoint` mesmo que os nomes das variáveis/escopos mudem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh61SGeqb27b"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a_different_name', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  b = tf1.get_variable('b_different_name', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.zeros_initializer())\n",
        "  with tf1.variable_scope('different_scope'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.zeros_initializer())\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    print(\"Initialized [a, b, c]: \", sess.run([a, b, c]))\n",
        "\n",
        "    ckpt = tf.train.Checkpoint(unscoped=[a, b], scoped=[c])\n",
        "    # `assert_consumed` validates that all checkpoint objects are restored from\n",
        "    # the checkpoint. `run_restore_ops` is required when running in a TF1\n",
        "    # session.\n",
        "    ckpt.restore(tf2_ckpt_path).assert_consumed().run_restore_ops()\n",
        "\n",
        "    # Removing `assert_consumed` is fine if you want to skip the validation.\n",
        "    # ckpt.restore(tf2_ckpt_path).run_restore_ops()\n",
        "\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unDPmL-kldr2"
      },
      "source": [
        "E no modo eager:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79S0zMAnfzx7"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "c = tf.Variable(0.)\n",
        "print(\"Initialized [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])\n",
        "\n",
        "# The keys \"scoped\" and \"unscoped\" are no longer relevant, but are used to\n",
        "# maintain compatibility with the saved checkpoints.\n",
        "ckpt = tf.train.Checkpoint(unscoped=[a, b], scoped=[c])\n",
        "\n",
        "ckpt.restore(tf2_ckpt_path).assert_consumed().run_restore_ops()\n",
        "print(\"Restored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKfNAr8l3aFg"
      },
      "source": [
        "## Checkpoints TF2 no Estimator\n",
        "\n",
        "As seções acima descrevem como manter a compatibilidade do checkpoint ao migrar seu modelo. Esses conceitos também se aplicam aos modelos Estimator, embora a maneira como o checkpoint é salvo/carregado seja um pouco diferente. Ao migrar seu modelo Estimator para usar APIs TF2, você pode querer trocar os checkpoints TF1 para TF2 *enquanto o modelo ainda estiver usando o estimador*. Esta seção mostra como fazer isso.\n",
        "\n",
        "O [`tf.estimator.Estimator`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) e o [`MonitoredSession`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MonitoredSession) possuem um mecanismo de salvamento chamado `scaffold`, um objeto [`tf.compat.v1.train.Scaffold`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Scaffold). O `Scaffold` pode conter um `tf1.train.Saver` ou `tf.train.Checkpoint`, que permite que o `Estimator` e o `MonitoredSession` salvem checkpoint no estilo TF1 ou TF2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8AT_oO-5TXU"
      },
      "outputs": [],
      "source": [
        "# A model_fn that saves a TF1 checkpoint\n",
        "def model_fn_tf1_ckpt(features, labels, mode):\n",
        "  # This model adds 2 to the variable `v` in every train step.\n",
        "  train_step = tf1.train.get_or_create_global_step()\n",
        "  v = tf1.get_variable('var', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode,\n",
        "      predictions=v,\n",
        "      train_op=tf.group(v.assign_add(2), train_step.assign_add(1)),\n",
        "      loss=tf.constant(1.),\n",
        "      scaffold=None\n",
        "  )\n",
        "\n",
        "!rm -rf est-tf1\n",
        "est = tf.estimator.Estimator(model_fn_tf1_ckpt, 'est-tf1')\n",
        "\n",
        "def train_fn():\n",
        "  return tf.data.Dataset.from_tensor_slices(([1,2,3], [4,5,6]))\n",
        "est.train(train_fn, steps=1)\n",
        "\n",
        "latest_checkpoint = tf.train.latest_checkpoint('est-tf1')\n",
        "print_checkpoint(latest_checkpoint)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttH6cUrl7jK2"
      },
      "outputs": [],
      "source": [
        "# A model_fn that saves a TF2 checkpoint\n",
        "def model_fn_tf2_ckpt(features, labels, mode):\n",
        "  # This model adds 2 to the variable `v` in every train step.\n",
        "  train_step = tf1.train.get_or_create_global_step()\n",
        "  v = tf1.get_variable('var', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  ckpt = tf.train.Checkpoint(var_list={'var': v}, step=train_step)\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode,\n",
        "      predictions=v,\n",
        "      train_op=tf.group(v.assign_add(2), train_step.assign_add(1)),\n",
        "      loss=tf.constant(1.),\n",
        "      scaffold=tf1.train.Scaffold(saver=ckpt)\n",
        "  )\n",
        "\n",
        "!rm -rf est-tf2\n",
        "est = tf.estimator.Estimator(model_fn_tf2_ckpt, 'est-tf2',\n",
        "                             warm_start_from='est-tf1')\n",
        "\n",
        "def train_fn():\n",
        "  return tf.data.Dataset.from_tensor_slices(([1,2,3], [4,5,6]))\n",
        "est.train(train_fn, steps=1)\n",
        "\n",
        "latest_checkpoint = tf.train.latest_checkpoint('est-tf2')\n",
        "print_checkpoint(latest_checkpoint)  \n",
        "\n",
        "assert est.get_variable_value('var_list/var/.ATTRIBUTES/VARIABLE_VALUE') == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYVYgahE8daL"
      },
      "source": [
        "O valor final de `v` deve ser `16`, após ser inicializado com warm start em `est-tf1`, e depois treinado por mais 5 passos. O valor do passo 'train' não é transferido do checkpoint `warm_start`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq8EjblQUIA2"
      },
      "source": [
        "## Checkpoints no Keras\n",
        "\n",
        "Modelos construídos com Keras ainda usam `tf1.train.Saver` e `tf.train.Checkpoint` para carregar pesos pré-existentes. Quando seu modelo estiver totalmente migrado, passe a usar `model.save_weights` e `model.load_weights`, especialmente se estiver usando o callback `ModelCheckpoint` durante o treinamento.\n",
        "\n",
        "Algumas coisas que você deve saber sobre checkpoints e Keras:\n",
        "\n",
        "**Inicialização vs Construção**\n",
        "\n",
        "Os modelos e camadas do Keras devem passar por **dois passos** antes de serem totalmente criados. O primeiro é a *inicialização* (initialization) do objeto Python: `layer = tf.keras.layers.Dense(x)`. O segundo é o passo de *construção* (build), no qual a maioria dos pesos é de fato criada: `layer.build(input_shape)`. Você também pode criar um modelo chamando-o ou executando um único passo `train` , `eval` ou `predict` (somente na primeira vez).\n",
        "\n",
        "Se você descobrir que `model.load_weights(path).assert_consumed()` está gerando um erro, é provável que o modelo/camadas não tenham sido construídos.\n",
        "\n",
        "**Keras usa checkpoints TF2**\n",
        "\n",
        "`tf.train.Checkpoint(model).write` é equivalente a `model.save_weights`. O mesmo vale para `tf.train.Checkpoint(model).read` e `model.load_weights`. Observe que `Checkpoint(model) != Checkpoint(model=model)`.\n",
        "\n",
        "**Os checkpoints do TF2 funcionam com o passo `build()` do Keras**\n",
        "\n",
        "`tf.train.Checkpoint.restore` tem um mecanismo chamado *restauração adiada* (deferred restoration) que permite que objetos `tf.Module` e Keras armazenem valores de variáveis ​​se a variável ainda não tiver sido criada. Isso permite a modelos *inicializados* carregar pesos e *construir* (build) depois.\n",
        "\n",
        "```\n",
        "m = YourKerasModel()\n",
        "status = m.load_weights(path)\n",
        "\n",
        "# This call builds the model. The variables are created with the restored\n",
        "# values.\n",
        "m.predict(inputs)\n",
        "\n",
        "status.assert_consumed()\n",
        "```\n",
        "\n",
        "Devido a esse mecanismo, é altamente recomendável usar APIs de carregamento de checkpoints TF2 com modelos Keras (mesmo ao restaurar checkpoints TF1 pré-existentes nos [shims de mapeamento de modelo](./model_mapping.ipynb)). Veja mais no [guia de checkpoints](https://www.tensorflow.org/guide/checkpoint#delayed_restorations).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO2NucRtqMm6"
      },
      "source": [
        "## Amostras de codigo\n",
        "\n",
        "As amostras de código abaixo demonstram a compatibilidade das versões TF1/TF2 nas APIs de salvamento de checkpoints. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3SSc74olkX3"
      },
      "source": [
        "### Salvando um checkpoint TF1 em TF2\n",
        "\n",
        "<a name=\"save-tf1-in-tf2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2ZPk8BPloE1"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(1.0, name='a')\n",
        "b = tf.Variable(2.0, name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(3.0, name='c')\n",
        "\n",
        "saver = tf1.train.Saver(var_list=[a, b, c])\n",
        "path = saver.save(sess=None, save_path='tf1-ckpt-saved-in-eager')\n",
        "print_checkpoint(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxyN5khVjhmA"
      },
      "source": [
        "### Carregando um checkpoint TF1 em TF2\n",
        "\n",
        "<a name=\"load-tf1-in-tf2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5kSXy3FmA79"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0., name='a')\n",
        "b = tf.Variable(0., name='b')\n",
        "with tf.name_scope('scoped'):\n",
        "  c = tf.Variable(0., name='c')\n",
        "print(\"Initialized [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])\n",
        "saver = tf1.train.Saver(var_list=[a, b, c])\n",
        "saver.restore(sess=None, save_path='tf1-ckpt-saved-in-eager')\n",
        "print(\"Restored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul3V4pEwloeN"
      },
      "source": [
        "### Salvando um checkpoint TF2 em TF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhuP_2EIlRm4"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(1))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(2))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(3))\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    ckpt = tf.train.Checkpoint(\n",
        "        var_list={v.name.split(':')[0]: v for v in tf1.global_variables()})\n",
        "    tf2_in_tf1_path = ckpt.save('tf2-ckpt-saved-in-session')\n",
        "    print_checkpoint(tf2_in_tf1_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiViCjCDgxhz"
      },
      "source": [
        "### Carregando um checkpoint TF2 em TF1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-4hIPZvmXlb"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(0))\n",
        "  with tf1.Session() as sess:\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    print(\"Initialized [a, b, c]: \", sess.run([a, b, c]))\n",
        "    ckpt = tf.train.Checkpoint(\n",
        "        var_list={v.name.split(':')[0]: v for v in tf1.global_variables()})\n",
        "    ckpt.restore('tf2-ckpt-saved-in-session-1').run_restore_ops()\n",
        "    print(\"Restored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRrSE2X6sgAM"
      },
      "source": [
        "## Conversão de checkpoints\n",
        "\n",
        "<a name=\"checkpoint-conversion\"></a>\n",
        "\n",
        "Você pode converter checkpoints entre TF1 e TF2 carregando-os e salvando-os novamente. Uma alternativa é `tf.train.load_checkpoint`, mostrada no código abaixo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9KByaLous4q"
      },
      "source": [
        "### Conversão de checkpoints TF1 para TF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG8grCv2smAb"
      },
      "outputs": [],
      "source": [
        "def convert_tf1_to_tf2(checkpoint_path, output_prefix):\n",
        "  \"\"\"Converts a TF1 checkpoint to TF2.\n",
        "\n",
        "  To load the converted checkpoint, you must build a dictionary that maps\n",
        "  variable names to variable objects.\n",
        "  ```\n",
        "  ckpt = tf.train.Checkpoint(vars={name: variable})  \n",
        "  ckpt.restore(converted_ckpt_path)\n",
        "  ```\n",
        "\n",
        "  Args:\n",
        "    checkpoint_path: Path to the TF1 checkpoint.\n",
        "    output_prefix: Path prefix to the converted checkpoint.\n",
        "\n",
        "  Returns:\n",
        "    Path to the converted checkpoint.\n",
        "  \"\"\"\n",
        "  vars = {}\n",
        "  reader = tf.train.load_checkpoint(checkpoint_path)\n",
        "  dtypes = reader.get_variable_to_dtype_map()\n",
        "  for key in dtypes.keys():\n",
        "    vars[key] = tf.Variable(reader.get_tensor(key))\n",
        "  return tf.train.Checkpoint(vars=vars).save(output_prefix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyvqK6Sb3dad"
      },
      "source": [
        "Converta o checkpoint salvo na amostra de código `Salvando um checkpoint TF2 em TF1`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcHLN4lPvYvw"
      },
      "outputs": [],
      "source": [
        "# Make sure to run the snippet in `Save a TF1 checkpoint in TF2`.\n",
        "print_checkpoint('tf1-ckpt-saved-in-eager')\n",
        "converted_path = convert_tf1_to_tf2('tf1-ckpt-saved-in-eager', \n",
        "                                     'converted-tf1-to-tf2')\n",
        "print(\"\\n[Converted]\")\n",
        "print_checkpoint(converted_path)\n",
        "\n",
        "# Try loading the converted checkpoint.\n",
        "a = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "c = tf.Variable(0.)\n",
        "ckpt = tf.train.Checkpoint(vars={'a': a, 'b': b, 'scoped/c': c})\n",
        "ckpt.restore(converted_path).assert_consumed()\n",
        "print(\"\\nRestored [a, b, c]: \", [a.numpy(), b.numpy(), c.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fokg6ybZvE20"
      },
      "source": [
        "### Conversão de checkpoints TF2 para TF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPQsXQveuQiC"
      },
      "outputs": [],
      "source": [
        "def convert_tf2_to_tf1(checkpoint_path, output_prefix):\n",
        "  \"\"\"Converts a TF2 checkpoint to TF1.\n",
        "\n",
        "  The checkpoint must be saved using a \n",
        "  `tf.train.Checkpoint(var_list={name: variable})`\n",
        "\n",
        "  To load the converted checkpoint with `tf.compat.v1.Saver`:\n",
        "  ```\n",
        "  saver = tf.compat.v1.train.Saver(var_list={name: variable}) \n",
        "\n",
        "  # An alternative, if the variable names match the keys:\n",
        "  saver = tf.compat.v1.train.Saver(var_list=[variables]) \n",
        "  saver.restore(sess, output_path)\n",
        "  ```\n",
        "  \"\"\"\n",
        "  vars = {}\n",
        "  reader = tf.train.load_checkpoint(checkpoint_path)\n",
        "  dtypes = reader.get_variable_to_dtype_map()\n",
        "  for key in dtypes.keys():\n",
        "    # Get the \"name\" from the \n",
        "    if key.startswith('var_list/'):\n",
        "      var_name = key.split('/')[1]\n",
        "      # TF2 checkpoint keys use '/', so if they appear in the user-defined name,\n",
        "      # they are escaped to '.S'.\n",
        "      var_name = var_name.replace('.S', '/')\n",
        "      vars[var_name] = tf.Variable(reader.get_tensor(key))\n",
        "  \n",
        "  return tf1.train.Saver(var_list=vars).save(sess=None, save_path=output_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjZD_OSf1mKX"
      },
      "source": [
        "Converta o checkpoint salvo no trecho de código `Salvando um checkpoint TF2 em TF1`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc1MVeV6z2DB"
      },
      "outputs": [],
      "source": [
        "# Make sure to run the snippet in `Save a TF2 checkpoint in TF1`.\n",
        "print_checkpoint('tf2-ckpt-saved-in-session-1')\n",
        "converted_path = convert_tf2_to_tf1('tf2-ckpt-saved-in-session-1',\n",
        "                                    'converted-tf2-to-tf1')\n",
        "print(\"\\n[Converted]\")\n",
        "print_checkpoint(converted_path)\n",
        "\n",
        "# Try loading the converted checkpoint.\n",
        "with tf.Graph().as_default() as g:\n",
        "  a = tf1.get_variable('a', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  b = tf1.get_variable('b', shape=[], dtype=tf.float32, \n",
        "                       initializer=tf1.constant_initializer(0))\n",
        "  with tf1.variable_scope('scoped'):\n",
        "    c = tf1.get_variable('c', shape=[], dtype=tf.float32, \n",
        "                        initializer=tf1.constant_initializer(0))\n",
        "  with tf1.Session() as sess:\n",
        "    saver = tf1.train.Saver([a, b, c])\n",
        "    saver.restore(sess, converted_path)\n",
        "    print(\"\\nRestored [a, b, c]: \", sess.run([a, b, c]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBMfArLQ0jb-"
      },
      "source": [
        "## Guias Relacionados\n",
        "\n",
        "- [Validando equivalência numérica e exatidão](./validate_correctness.ipynb)\n",
        "- [Guia de mapeamento de modelos](./model_mapping.ipynb) e `tf.compat.v1.keras.utils.track_tf1_style_variables`\n",
        "- [Guia de checkpoints no TF2](https://www.tensorflow.org/guide/checkpoint)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "migrating_checkpoints.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
