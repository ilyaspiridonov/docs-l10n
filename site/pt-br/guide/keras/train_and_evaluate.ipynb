{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb291b62b1aa"
      },
      "source": [
        "# Treinamento e avaliação com os métodos integrados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1820d9bdfb9"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte no GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4ac441b1fc"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0472bf67b2bf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfdc6f08988e"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "Este guia trata de modelos de treinamento, avaliação e previsão (inferência) ao usar APIs integradas para treinamento e validação (tais como `Model.fit()`, `Model.evaluate()` e `Model.predict()`).\n",
        "\n",
        "Se você tiver interesse em utilizar `fit()` ao especificar sua própria função de passo de treinamento, consulte o <a href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/\" data-md-type=\"link\">guia Personalizando o que acontece no `fit()`</a>.\n",
        "\n",
        "Se seu interesse for escrever seus próprios loops de treinamento e avaliação do zero, consulte o guia [\"escrevendo um loop de treinamento do zero\"](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/).\n",
        "\n",
        "Em geral, esteja você usando loops integrados ou escrevendo seus próprios loops, o treinamento e a avaliação do modelo funcionam estritamente da mesma maneira em todos os tipos de modelo Keras:  modelos sequenciais, modelos construídos com a API Functional e modelos escritos do zero através da implementação da subclasse de um modelo.\n",
        "\n",
        "Este guia não trata do treinamento distribuído, que é abordado em nosso [guia para treinamento multi-GPU e distribuído](https://keras.io/guides/distributed_training/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e270faa413e"
      },
      "source": [
        "## Visão geral da API: um primeiro exemplo completo\n",
        "\n",
        "Ao passar dados para os loops de treinamento integrados de um modelo, você deve ou usar **arrays NumPy** (se seus dados forem pequenos e caberem na memória), ou **`tf.data Dataset` objects**. Nos próximos parágrafos, usaremos o dataset MNIST como matrizes NumPy, a fim de demonstrar como usar otimizadores, perdas e métricas.\n",
        "\n",
        "Vamos considerar o seguinte modelo (aqui, construímos com a API funcional, mas também poderia ser um modelo sequencial ou de subclasse):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "170a6a18b2a3"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6d5724a90ab"
      },
      "source": [
        "Um típico workflow de ponta a ponta consistiria de:\n",
        "\n",
        "- Treinamento\n",
        "- Validação num conjunto de validação gerado a partir dos dados de treinamento originais\n",
        "- Avaliação nos dados de teste\n",
        "\n",
        "Usaremos dados MNIST para este exemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b55b3903edb"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data (these are NumPy arrays)\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
        "\n",
        "y_train = y_train.astype(\"float32\")\n",
        "y_test = y_test.astype(\"float32\")\n",
        "\n",
        "# Reserve 10,000 samples for validation\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77a84eb1985b"
      },
      "source": [
        "Especificamos a configuração de treinamento (otimizador, perda, métricas):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26a7f1819796"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
        "    # Loss function to minimize\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    # List of metrics to monitor\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58dc05fa2736"
      },
      "source": [
        "Chamamos `fit()`, que treinará o modelo dividindo os dados em \"lotes\" de tamanho `batch_size` e iterando repetidamente em todo o dataset por um determinado número de `epochs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b92f67b105e"
      },
      "outputs": [],
      "source": [
        "print(\"Fit model on training data\")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=2,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    validation_data=(x_val, y_val),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "896edfc3d7c4"
      },
      "source": [
        "O objeto `history` retornado contém um registro dos valores de perda e valores de métrica durante o treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a20b8f5b9fcc"
      },
      "outputs": [],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6105b646df66"
      },
      "source": [
        "Avaliamos o modelo nos dados de teste via `evaluate()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69f524a93f9d"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "# Generate predictions (probabilities -- the output of the last layer)\n",
        "# on new data using `predict`\n",
        "print(\"Generate predictions for 3 samples\")\n",
        "predictions = model.predict(x_test[:3])\n",
        "print(\"predictions shape:\", predictions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f19d074eb88c"
      },
      "source": [
        "Agora, vamos revisar cada parte desse workflow em detalhes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3669f026d14"
      },
      "source": [
        "## O método `compile()`: especificando uma perda, métricas e um otimizador\n",
        "\n",
        "Para treinar um modelo com `fit()`, você precisa especificar uma função de perda, um otimizador e, opcionalmente, algumas métricas a serem monitoradas.\n",
        "\n",
        "Você passa esses parâmetros para o modelo como argumentos do método `compile()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb7a8deb494c"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b934b428dc43"
      },
      "source": [
        "O argumento `metrics` deve ser uma lista: seu modelo pode ter qualquer quantidade de métricas.\n",
        "\n",
        "Se seu modelo tiver múltiplas saídas, você poderá especificar diferentes perdas e métricas para cada saída e modular a contribuição de cada saída para a perda total do modelo. Você encontrará mais detalhes sobre isso na seção **Passando dados para modelos multi-entrada, multi-saída**.\n",
        "\n",
        "Observe que, se você estiver satisfeito com as configurações padrão, em muitos casos, o otimizador, a perda e as métricas podem ser especificados via identificadores de string como um atalho:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6444839ff300"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5493ab963254"
      },
      "source": [
        "Para reutilização posterior, vamos colocar nossa definição de modelo e passo de compilação dentro de funções; vamos chamá-las várias vezes em diferentes exemplos neste guia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31c3e3c70f06"
      },
      "outputs": [],
      "source": [
        "def get_uncompiled_model():\n",
        "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_compiled_model():\n",
        "    model = get_uncompiled_model()\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"sparse_categorical_accuracy\"],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "535137cf19b2"
      },
      "source": [
        "### Muitos otimizadores, perdas e métricas integrados estão disponíveis na API\n",
        "\n",
        "Em geral, você não terá que criar suas próprias perdas, métricas ou otimizadores do zero, porque o que você precisa provavelmente já faz parte da API do Keras:\n",
        "\n",
        "Otimizadores:\n",
        "\n",
        "- `SGD()` (com ou sem momento)\n",
        "- `RMSprop()`\n",
        "- `Adam()`\n",
        "- etc.\n",
        "\n",
        "Perdas:\n",
        "\n",
        "- `MeanSquaredError()`\n",
        "- `KLDivergence()`\n",
        "- `CosineSimilarity()`\n",
        "- etc.\n",
        "\n",
        "Métricas:\n",
        "\n",
        "- `AUC()`\n",
        "- `Precision()`\n",
        "- `Recall()`\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdc4c3d72a21"
      },
      "source": [
        "### Perdas personalizadas\n",
        "\n",
        "Se você precisar criar uma perda personalizada, o Keras oferece duas maneiras de fazer isso.\n",
        "\n",
        "O primeiro método envolve a criação de uma função que aceita entradas `y_true` e `y_pred`. O exemplo a seguir mostra uma função de perda que calcula o erro quadrático médio entre os dados reais e as previsões:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc4edd47bb5a"
      },
      "outputs": [],
      "source": [
        "def custom_mean_squared_error(y_true, y_pred):\n",
        "    return tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "\n",
        "model = get_uncompiled_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\n",
        "\n",
        "# We need to one-hot encode the labels to use MSE\n",
        "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
        "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25b9fa7941ca"
      },
      "source": [
        "Se você precisar de uma função de perda que aceite parâmetros além de `y_true` e `y_pred`, você pode criar uma subclasse da classe `tf.keras.losses.Loss` e implementar os dois métodos a seguir:\n",
        "\n",
        "- `__init__(self)`: aceita parâmetros a serem passados durante a chamada de sua função de perda\n",
        "- `call(self, y_true, y_pred)`: usa os alvos (y_true) e as previsões do modelo (y_pred) para calcular a perda do modelo\n",
        "\n",
        "Vamos supor que você queira usar o erro quadrático médio, mas com um termo adicionado que desincentivará os valores de previsão distantes de 0,5 (presumimos que os alvos categóricos são one-hot encoded e assumem valores entre 0 e 1). Isto cria um incentivo para que o modelo não seja muito confiante, o que pode ajudar a reduzir o overfitting (não saberemos se funciona até tentarmos!).\n",
        "\n",
        "Veja como você faria:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b09463a8c568"
      },
      "outputs": [],
      "source": [
        "class CustomMSE(keras.losses.Loss):\n",
        "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
        "        super().__init__(name=name)\n",
        "        self.regularization_factor = regularization_factor\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
        "        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
        "        return mse + reg * self.regularization_factor\n",
        "\n",
        "\n",
        "model = get_uncompiled_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n",
        "\n",
        "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
        "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2d7d358b4eb"
      },
      "source": [
        "### Métricas personalizadas\n",
        "\n",
        "Se você precisar de uma métrica que não faz parte da API, poderá criar facilmente métricas personalizadas criando uma subclasse da classe `tf.keras.metrics.Metric`. Você precisará implementar 4 métodos:\n",
        "\n",
        "- `__init__(self)`, no qual você criará variáveis ​​de estado para sua métrica.\n",
        "- `update_state(self, y_true, y_pred, sample_weight=None)`, que usa os alvos y_true e as previsões do modelo y_pred para atualizar as variáveis ​​de estado.\n",
        "- `result(self)`, que usa as variáveis ​​de estado para calcular os resultados finais.\n",
        "- `reset_state(self)`, que reinicializa o estado da métrica.\n",
        "\n",
        "A atualização de estado e a computação de resultados são mantidos separados (em `update_state()` e `result()`, respectivamente) porque, em alguns casos, a computação de resultados pode ser muito cara e só seria feita periodicamente.\n",
        "\n",
        "Aqui está um exemplo simples mostrando como implementar uma métrica `CategoricalTruePositives` que conta quantas amostras foram classificadas corretamente como pertencentes a uma determinada classe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ad9c57c0683"
      },
      "outputs": [],
      "source": [
        "class CategoricalTruePositives(keras.metrics.Metric):\n",
        "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
        "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
        "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
        "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
        "        values = tf.cast(values, \"float32\")\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
        "            values = tf.multiply(values, sample_weight)\n",
        "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
        "\n",
        "    def result(self):\n",
        "        return self.true_positives\n",
        "\n",
        "    def reset_state(self):\n",
        "        # The state of the metric will be reset at the start of each epoch.\n",
        "        self.true_positives.assign(0.0)\n",
        "\n",
        "\n",
        "model = get_uncompiled_model()\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[CategoricalTruePositives()],\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1547a2d92f6a"
      },
      "source": [
        "### Lidando com perdas e métricas que não se encaixam na assinatura padrão\n",
        "\n",
        "A esmagadora maioria das perdas e métricas pode ser calculada a partir de `y_true` e `y_pred`, onde `y_pred` é uma saída do seu modelo, mas isto não é verdade para todas. Por exemplo, uma perda de regularização pode requerer apenas a ativação de uma camada (não há alvos neste caso), e esta ativação pode não ser uma saída do modelo.\n",
        "\n",
        "Em tais casos, você pode chamar `self.add_loss(loss_value)` de dentro do método call de uma camada personalizada. As perdas adicionadas dessa maneira são adicionadas à perda \"principal\" durante o treinamento (a que é passada para `compile()`). Aqui está um exemplo simples que adiciona regularização de atividades (observe que a regularização de atividades é integrada em todas as camadas do Keras: esta camada serve apenas para fornecer um exemplo concreto):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b494d47437a0"
      },
      "outputs": [],
      "source": [
        "class ActivityRegularizationLayer(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
        "        return inputs  # Pass-through layer.\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "\n",
        "# Insert activity regularization as a layer\n",
        "x = ActivityRegularizationLayer()(x)\n",
        "\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "# The displayed loss will be much higher than before\n",
        "# due to the regularization component.\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaebb5829011"
      },
      "source": [
        "Você pode fazer o mesmo para registrar em log valores de métricas, usando `add_metric()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa58091be092"
      },
      "outputs": [],
      "source": [
        "class MetricLoggingLayer(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        # The `aggregation` argument defines\n",
        "        # how to aggregate the per-batch values\n",
        "        # over each epoch:\n",
        "        # in this case we simply average them.\n",
        "        self.add_metric(\n",
        "            keras.backend.std(inputs), name=\"std_of_activation\", aggregation=\"mean\"\n",
        "        )\n",
        "        return inputs  # Pass-through layer.\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "\n",
        "# Insert std logging as a layer.\n",
        "x = MetricLoggingLayer()(x)\n",
        "\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3c18154d057"
      },
      "source": [
        "Na [API Functional](https://www.tensorflow.org/guide/keras/functional/) , você também pode chamar `model.add_loss(loss_tensor)`, ou `model.add_metric(metric_tensor, name, aggregation)`.\n",
        "\n",
        "Eis aqui um exemplo simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e19afe78b3a"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x1)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
        "\n",
        "model.add_metric(keras.backend.std(x1), name=\"std_of_activation\", aggregation=\"mean\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b06d48035369"
      },
      "source": [
        "Observe que quando você passa perdas via `add_loss()`, torna-se possível chamar `compile()` sem uma função de perda, pois o modelo já possui uma perda a minimizar.\n",
        "\n",
        "Considere a seguinte camada `LogisticEndpoint`: ela usa como entrada alvos e logits e rastreia uma perda de entropia cruzada via `add_loss()`. Ela também rastreia a exatidão da classificação via `add_metric()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d56d2c504258"
      },
      "outputs": [],
      "source": [
        "class LogisticEndpoint(keras.layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super(LogisticEndpoint, self).__init__(name=name)\n",
        "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    def call(self, targets, logits, sample_weights=None):\n",
        "        # Compute the training-time loss value and add it\n",
        "        # to the layer using `self.add_loss()`.\n",
        "        loss = self.loss_fn(targets, logits, sample_weights)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # Log accuracy as a metric and add it\n",
        "        # to the layer using `self.add_metric()`.\n",
        "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
        "        self.add_metric(acc, name=\"accuracy\")\n",
        "\n",
        "        # Return the inference-time prediction tensor (for `.predict()`).\n",
        "        return tf.nn.softmax(logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0698f3c98cbe"
      },
      "source": [
        "Você pode usá-la num modelo com duas entradas (dados de entrada e alvos), compilados sem um argumento `loss`, assim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f6842f2bbe6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
        "targets = keras.Input(shape=(10,), name=\"targets\")\n",
        "logits = keras.layers.Dense(10)(inputs)\n",
        "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
        "\n",
        "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
        "model.compile(optimizer=\"adam\")  # No loss argument!\n",
        "\n",
        "data = {\n",
        "    \"inputs\": np.random.random((3, 3)),\n",
        "    \"targets\": np.random.random((3, 10)),\n",
        "}\n",
        "model.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328b021aa6b8"
      },
      "source": [
        "Para obter mais informações sobre como treinar modelos multi-entradas, consulte a seção **Passando dados para modelos multi-entradas e multi-saídas**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9e6ea0045c9"
      },
      "source": [
        "### Separando automaticamente um conjunto de validação\n",
        "\n",
        "No primeiro exemplo completo que você viu, usamos o argumento `validation_data` para passar uma tupla de matrizes NumPy `(x_val, y_val)` para o modelo para avaliar uma perda de validação e métricas de validação no final de cada época.\n",
        "\n",
        "Aqui está outra alternativa: o argumento `validation_split` permite que você reserve automaticamente parte de seus dados de treinamento para validação. O valor do argumento representa a fração dos dados a serem reservados para validação, portanto, deve ser definido como um número maior que 0 e menor que 1. Por exemplo, `validation_split=0.2` significa \"usar 20% dos dados para validação\" e `validation_split=0.6` significa \"usar 60% dos dados para validação\".\n",
        "\n",
        "A forma como a validação é calculada é tomando as últimas x% amostras das matrizes recebidas pela chamada `fit()`, antes de qualquer embaralhamento.\n",
        "\n",
        "Observe que você só pode usar `validation_split` ao treinar com dados NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "232fd59c751b"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "770d19613c53"
      },
      "source": [
        "## Treinamento e avaliação de datasets tf.data\n",
        "\n",
        "Nos últimos parágrafos, você viu como lidar com perdas, métricas e otimizadores, e viu como usar os argumentos `validation_data` e `validation_split` em `fit()`, quando seus dados são passados ​​como arrays NumPy.\n",
        "\n",
        "Vamos agora dar uma olhada no caso em que seus dados vêm na forma de um objeto `tf.data.Dataset`.\n",
        "\n",
        "A API `tf.data` é um conjunto de utilitários no TensorFlow 2.0 para carregar e pré-processar dados de forma rápida e escalável.\n",
        "\n",
        "Para obter um guia completo sobre como criar `Datasets`, consulte a [documentação do tf.data](https://www.tensorflow.org/guide/data).\n",
        "\n",
        "Você pode passar uma instância `Dataset` diretamente para os métodos `fit()`, `evaluate()` e `predict()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bf4ded224f8"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# First, let's create a training Dataset instance.\n",
        "# For the sake of our example, we'll use the same MNIST data as before.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "# Shuffle and slice the dataset.\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Now we get a test dataset.\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(64)\n",
        "\n",
        "# Since the dataset already takes care of batching,\n",
        "# we don't pass a `batch_size` argument.\n",
        "model.fit(train_dataset, epochs=3)\n",
        "\n",
        "# You can also evaluate or predict on a dataset.\n",
        "print(\"Evaluate\")\n",
        "result = model.evaluate(test_dataset)\n",
        "dict(zip(model.metrics_names, result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "421d16914ce3"
      },
      "source": [
        "Observe que o Dataset é reinicializado no final de cada época, para que possa ser reutilizado na época seguinte.\n",
        "\n",
        "Se você quiser executar o treinamento apenas num número específico de lotes deste dataset, pode passar o argumento `steps_per_epoch`, que especifica quantos passos de treinamento o modelo deve executar usando este dataset antes de passar para a época seguinte.\n",
        "\n",
        "Se você fizer isso, o dataset não será reinicializado no final de cada época; em vez disso, simplesmente continuaremos buscando os próximos lotes. O dataset acabará ficando sem dados (a menos que seja um dataset com loop infinito)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "273c5dff16b4"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Only use the 100 batches per epoch (that's 64 * 100 samples)\n",
        "model.fit(train_dataset, epochs=3, steps_per_epoch=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2dcd180da7b"
      },
      "source": [
        "### Usando um dataset de validação\n",
        "\n",
        "Você pode passar uma instância `Dataset` como o `validation_data` em `fit()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf4f3d78e69a"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Prepare the validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(64)\n",
        "\n",
        "model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7f0ebf5f1d"
      },
      "source": [
        "No final de cada época, o modelo irá iterar sobre o dataset de validação e calcular a perda de validação e as métricas de validação.\n",
        "\n",
        "Se você quiser executar a validação apenas num número específico de lotes desse conjunto de dados, pode passar o `validation_steps`, que especifica quantos passos de validação o modelo deve executar com o dataset de validação antes de interromper a validação e passar para a época seguinte:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f47342fed069"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Prepare the validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(64)\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=1,\n",
        "    # Only run validation using the first 10 batches of the dataset\n",
        "    # using the `validation_steps` argument\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67b4418e9f26"
      },
      "source": [
        "Observe que o dataset de validação será reinicializado após cada uso (para que você sempre avalie nas mesmas amostras de época para época).\n",
        "\n",
        "O argumento `validation_split` (gerando um conjunto de validação a partir dos dados de treinamento) não é suportado ao treinar a partir de objetos `Dataset`, pois esse recurso requer a capacidade de indexar as amostras dos datasets, o que geralmente não é possível com a API `Dataset`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8160beb766a0"
      },
      "source": [
        "## Outros formatos de entrada suportados\n",
        "\n",
        "Além de arrays NumPy, tensores eager e `Datasets` TensorFlow, é possível treinar um modelo Keras usando dataframes Pandas ou geradores Python que geram lotes de dados e rótulos.\n",
        "\n",
        "Em particular, a classe `keras.utils.Sequence` oferece uma interface simples para construir geradores de dados Python que reconhecem multiprocessamento e podem ser embaralhados.\n",
        "\n",
        "Em geral, recomendamos que você use:\n",
        "\n",
        "- Dados de entrada NumPy se seus dados forem pequenos e caberem na memória\n",
        "- Objetos `Dataset` se você tiver grandes datasets e precisar fazer treinamento distribuído\n",
        "- Objetos `Sequence` se você tiver grandes datasets e precisar fazer muitos processamentos personalizados do lado do Python que não podem ser feitos no TensorFlow (por exemplo, se você depender de bibliotecas externas para carregamento ou pré-processamento de dados).\n",
        "\n",
        "## Usando um objeto `keras.utils.Sequence` como entrada\n",
        "\n",
        "O `keras.utils.Sequence` é um utilitário a partir do qual você pode construir uma subclasse para obter um gerador Python com duas propriedades importantes:\n",
        "\n",
        "- Funciona bem com multiprocessamento.\n",
        "- Ele pode ser embaralhado (por exemplo, ao passar `shuffle=True` em `fit()`).\n",
        "\n",
        "Uma `Sequence` deve implementar dois métodos:\n",
        "\n",
        "- `__getitem__`\n",
        "- `__len__`\n",
        "\n",
        "O método `__getitem__` deve retornar um lote completo. Se você deseja modificar seu dataset entre épocas, pode implementar `on_epoch_end`.\n",
        "\n",
        "Eis aqui um exemplo rápido:\n",
        "\n",
        "```python\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "\n",
        "# Here, `filenames` is list of path to the images\n",
        "# and `labels` are the associated labels.\n",
        "\n",
        "class CIFAR10Sequence(Sequence):\n",
        "    def __init__(self, filenames, labels, batch_size):\n",
        "        self.filenames, self.labels = filenames, labels\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return np.array([\n",
        "            resize(imread(filename), (200, 200))\n",
        "               for filename in batch_x]), np.array(batch_y)\n",
        "\n",
        "sequence = CIFAR10Sequence(filenames, labels, batch_size)\n",
        "model.fit(sequence, epochs=10)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a28343b1967"
      },
      "source": [
        "## Usando ponderação de amostra e de classe\n",
        "\n",
        "Com as configurações padrão, o peso de uma amostra é decidido por sua frequência no dataset. Existem dois métodos para ponderar os dados, independentemente da frequência da amostra:\n",
        "\n",
        "- Pesos de classe\n",
        "- Pesos de amostra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f234a9a75b6d"
      },
      "source": [
        "### Pesos de classe\n",
        "\n",
        "Isso é definido passando um dicionário para o argumento `class_weight` para `Model.fit()`. Este dicionário mapeia índices de classe ao peso que deve ser usado para amostras pertencentes a esta classe.\n",
        "\n",
        "Isto pode ser usado para equilibrar classes sem reamostragem ou para treinar um modelo que dê mais importância a uma determinada classe.\n",
        "\n",
        "Por exemplo, se a classe \"0\" tiver metade da representação da classe \"1\" em seus dados, você poderá usar `Model.fit(..., class_weight={0: 1., 1: 0.5})`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9929d26d91b8"
      },
      "source": [
        "Aqui está um exemplo NumPy onde usamos pesos de classe ou pesos de amostra para dar mais importância à classificação correta da classe #5 (que é o dígito \"5\" no conjunto de dados MNIST)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1844f2329a6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class_weight = {\n",
        "    0: 1.0,\n",
        "    1: 1.0,\n",
        "    2: 1.0,\n",
        "    3: 1.0,\n",
        "    4: 1.0,\n",
        "    # Set weight \"2\" for class \"5\",\n",
        "    # making this class 2x more important\n",
        "    5: 2.0,\n",
        "    6: 1.0,\n",
        "    7: 1.0,\n",
        "    8: 1.0,\n",
        "    9: 1.0,\n",
        "}\n",
        "\n",
        "print(\"Fit with class weight\")\n",
        "model = get_compiled_model()\n",
        "model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce27221fad08"
      },
      "source": [
        "### Pesos de amostra\n",
        "\n",
        "Para controle refinado, ou se você não estiver construindo um classificador, você pode usar \"pesos de amostra\".\n",
        "\n",
        "- Ao treinar com dados NumPy: Passe o argumento `sample_weight` para `Model.fit()`.\n",
        "- Ao treinar com `tf.data` ou qualquer outro tipo de iterador: Forneça tuplas `(input_batch, label_batch, sample_weight_batch)`.\n",
        "\n",
        "Um array de \"pesos de amostra\" é um array de números que especifica quanto peso cada amostra em um lote deve ter no cálculo da perda total. É frequentemente usado em problemas de classificação desbalanceados (a ideia é dar mais peso a classes raramente vistas).\n",
        "\n",
        "Quando os pesos utilizados são uns e zeros, o array pode ser utilizado como uma *máscara* para a função de perda (descartando totalmente a contribuição de determinadas amostras para a perda total)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9819d647793"
      },
      "outputs": [],
      "source": [
        "sample_weight = np.ones(shape=(len(y_train),))\n",
        "sample_weight[y_train == 5] = 2.0\n",
        "\n",
        "print(\"Fit with sample weight\")\n",
        "model = get_compiled_model()\n",
        "model.fit(x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eae5837c5f56"
      },
      "source": [
        "Aqui está um exemplo `Dataset` correspondente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c870f3f0c66c"
      },
      "outputs": [],
      "source": [
        "sample_weight = np.ones(shape=(len(y_train),))\n",
        "sample_weight[y_train == 5] = 2.0\n",
        "\n",
        "# Create a Dataset that includes sample weights\n",
        "# (3rd element in the return tuple).\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, sample_weight))\n",
        "\n",
        "# Shuffle and slice the dataset.\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "model = get_compiled_model()\n",
        "model.fit(train_dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5f94cd76df5"
      },
      "source": [
        "## Passando dados para modelos com multi-entradas e multi-saídas\n",
        "\n",
        "Nos exemplos anteriores, estávamos considerando um modelo com uma única entrada (um tensor de formato `(764,)`) e uma única saída (um tensor de previsão de forma `(10,)` ). Mas como ficam os modelos que possuem múltiplas entradas ou saídas?\n",
        "\n",
        "Considere o modelo a seguir, que tem uma entrada de imagem de formato `(32, 32, 3)` (isto é `(height, width, channels)`) e uma entrada de série temporal de formato `(None, 10)` (isto é `(timesteps, features)`). Nosso modelo terá duas saídas calculadas a partir da combinação dessas entradas: uma \"pontuação\" (de formato `(1,)`) e uma distribuição de probabilidade em cinco classes (de formato `(5,)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f958449a057"
      },
      "outputs": [],
      "source": [
        "image_input = keras.Input(shape=(32, 32, 3), name=\"img_input\")\n",
        "timeseries_input = keras.Input(shape=(None, 10), name=\"ts_input\")\n",
        "\n",
        "x1 = layers.Conv2D(3, 3)(image_input)\n",
        "x1 = layers.GlobalMaxPooling2D()(x1)\n",
        "\n",
        "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
        "x2 = layers.GlobalMaxPooling1D()(x2)\n",
        "\n",
        "x = layers.concatenate([x1, x2])\n",
        "\n",
        "score_output = layers.Dense(1, name=\"score_output\")(x)\n",
        "class_output = layers.Dense(5, name=\"class_output\")(x)\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs=[image_input, timeseries_input], outputs=[score_output, class_output]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df3ed34fe78b"
      },
      "source": [
        "Vamos plotar este modelo, para que você possa ver claramente o que estamos fazendo aqui (observe que os formatos mostrados no gráfico são formatos de lote, em vez de formatos por amostra)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac8c1baca9e3"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d979e89b335"
      },
      "source": [
        "Em tempo de compilação, podemos especificar diferentes perdas para diferentes saídas, passando as funções de perda como uma lista:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9655c0084d70"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5fc73405283"
      },
      "source": [
        "Se passássemos apenas uma única função de perda para o modelo, a mesma função de perda seria aplicada a todas as saídas (o que não é apropriado aqui).\n",
        "\n",
        "Da mesma forma para as métricas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4c0c6c564bc"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
        "    metrics=[\n",
        "        [\n",
        "            keras.metrics.MeanAbsolutePercentageError(),\n",
        "            keras.metrics.MeanAbsoluteError(),\n",
        "        ],\n",
        "        [keras.metrics.CategoricalAccuracy()],\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dd9fb0343cc"
      },
      "source": [
        "Já que demos nomes às nossas camadas de saída, também podemos especificar perdas e métricas por saída através de um dict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42cb75110fc3"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={\n",
        "        \"score_output\": keras.losses.MeanSquaredError(),\n",
        "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
        "    },\n",
        "    metrics={\n",
        "        \"score_output\": [\n",
        "            keras.metrics.MeanAbsolutePercentageError(),\n",
        "            keras.metrics.MeanAbsoluteError(),\n",
        "        ],\n",
        "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd95ac0dd8b"
      },
      "source": [
        "Recomendamos o uso de nomes e dicts explícitos se você tiver mais de 2 saídas.\n",
        "\n",
        "É possível atribuir diferentes pesos a diferentes perdas específicas de saída (por exemplo, pode-se desejar privilegiar a perda de \"pontuação\" em nosso exemplo, atribuindo em 2x a importância da perda de classe), usando o argumento `loss_weights`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23a71e5f5227"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={\n",
        "        \"score_output\": keras.losses.MeanSquaredError(),\n",
        "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
        "    },\n",
        "    metrics={\n",
        "        \"score_output\": [\n",
        "            keras.metrics.MeanAbsolutePercentageError(),\n",
        "            keras.metrics.MeanAbsoluteError(),\n",
        "        ],\n",
        "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
        "    },\n",
        "    loss_weights={\"score_output\": 2.0, \"class_output\": 1.0},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147b5f581c32"
      },
      "source": [
        "Você também poderia optar por não calcular uma perda para determinadas saídas, se essas saídas forem destinadas à previsão, mas não ao treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d51aa372ef4"
      },
      "outputs": [],
      "source": [
        "# List loss version\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[None, keras.losses.CategoricalCrossentropy()],\n",
        ")\n",
        "\n",
        "# Or dict loss version\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={\"class_output\": keras.losses.CategoricalCrossentropy()},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c00d5f56d3f0"
      },
      "source": [
        "Passar dados para um modelo multi-entrada ou multi-saída em `fit()` funciona de maneira semelhante à especificação de uma função de perda em compile: você pode passar **listas de arrays NumPy** (com mapeamento 1:1 para as saídas que receberam uma função de perda) ou **dicts mapeando nomes de saída para arrays NumPy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0539da84328b"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
        ")\n",
        "\n",
        "# Generate dummy NumPy data\n",
        "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
        "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
        "score_targets = np.random.random_sample(size=(100, 1))\n",
        "class_targets = np.random.random_sample(size=(100, 5))\n",
        "\n",
        "# Fit on lists\n",
        "model.fit([img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=1)\n",
        "\n",
        "# Alternatively, fit on dicts\n",
        "model.fit(\n",
        "    {\"img_input\": img_data, \"ts_input\": ts_data},\n",
        "    {\"score_output\": score_targets, \"class_output\": class_targets},\n",
        "    batch_size=32,\n",
        "    epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e53eda8e1399"
      },
      "source": [
        "Aqui está o caso de uso `Dataset`: da mesma forma que fizemos para os arrays NumPy, o `Dataset` deve retornar uma tupla de dicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4df41a12ed2c"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        {\"img_input\": img_data, \"ts_input\": ts_data},\n",
        "        {\"score_output\": score_targets, \"class_output\": class_targets},\n",
        "    )\n",
        ")\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "model.fit(train_dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c792cd43a4"
      },
      "source": [
        "## Usando callbacks\n",
        "\n",
        "Callbacks em Keras são objetos chamados em diferentes pontos durante o treinamento (no início de uma época, no final de um lote, no final de uma época, etc.). Eles podem ser usados ​​para implementar certos comportamentos, como:\n",
        "\n",
        "- Validação em diferentes pontos durante o treinamento (além da validação por época integrada)\n",
        "- Checkpointing do modelo em intervalos regulares ou quando ele excede um determinado limite de exatidão\n",
        "- Alterando a taxa de aprendizado do modelo quando o treinamento parecer estagnado\n",
        "- Fazendo o ajuste fino das camadas superiores quando o treinamento parecer estar estagnado\n",
        "- Enviando de notificações por e-mail ou mensagem instantânea quando o treinamento terminar ou quando um determinado limite de desempenho for excedido\n",
        "- etc.\n",
        "\n",
        "Callbacks podem ser passados ​​como uma lista para sua chamada para `fit()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15036ddbee42"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        # Stop training when `val_loss` is no longer improving\n",
        "        monitor=\"val_loss\",\n",
        "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
        "        min_delta=1e-2,\n",
        "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "        patience=2,\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15f5af3b6da9"
      },
      "source": [
        "### Há muitos callbacks disponíveis\n",
        "\n",
        "Existem muitos callbacks integrados ao Keras, como:\n",
        "\n",
        "- `ModelCheckpoint` : Salva o modelo periodicamente.\n",
        "- `EarlyStopping` : Interrompe o treinamento quando o treinamento não estiver mais melhorando as métricas de validação.\n",
        "- `TensorBoard` : escreve logs de modelo periodicamente que podem ser visualizados no [TensorBoard](https://www.tensorflow.org/tensorboard) (mais detalhes na seção \"Visualização\").\n",
        "- `CSVLogger`: transmite dados de perda e métricas para um arquivo CSV.\n",
        "- etc.\n",
        "\n",
        "Consulte a [documentação de callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/) para obter a lista completa.\n",
        "\n",
        "### Escrevendo seu próprio callback\n",
        "\n",
        "Você pode criar um callback personalizado estendendo a classe base `keras.callbacks.Callback`. Um callback tem acesso ao modelo associado através da propriedade de classe `self.model`.\n",
        "\n",
        "Não deixe de ler o [guia completo para escrever callbacks personalizados](https://www.tensorflow.org/guide/keras/custom_callback/).\n",
        "\n",
        "Aqui está um exemplo simples de salvamento de uma lista de valores de perda por lote durante o treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b265d36ce608"
      },
      "outputs": [],
      "source": [
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs):\n",
        "        self.per_batch_losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        self.per_batch_losses.append(logs.get(\"loss\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ee672524987"
      },
      "source": [
        "## Modelos de checkpoint\n",
        "\n",
        "Quando você está treinando o modelo em datasets relativamente grandes, é crucial salvar os checkpoints do seu modelo em intervalos frequentes.\n",
        "\n",
        "A maneira mais fácil de conseguir isso é com o callback `ModelCheckpoint`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83614be57725"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_loss` score has improved.\n",
        "        # The saved model name will include the current epoch.\n",
        "        filepath=\"mymodel_{epoch}\",\n",
        "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "        monitor=\"val_loss\",\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "model.fit(\n",
        "    x_train, y_train, epochs=2, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f6afa36950c"
      },
      "source": [
        "O callback `ModelCheckpoint` pode ser usado para implementar tolerância a falhas: a capacidade de reiniciar o treinamento a partir do último estado salvo do modelo caso o treinamento seja interrompido aleatoriamente. Aqui está um exemplo básico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27ce92b2ad58"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Prepare a directory to store all the checkpoints.\n",
        "checkpoint_dir = \"./ckpt\"\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "\n",
        "def make_or_restore_model():\n",
        "    # Either restore the latest model, or create a fresh one\n",
        "    # if there is no checkpoint available.\n",
        "    checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]\n",
        "    if checkpoints:\n",
        "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
        "        print(\"Restoring from\", latest_checkpoint)\n",
        "        return keras.models.load_model(latest_checkpoint)\n",
        "    print(\"Creating a new model\")\n",
        "    return get_compiled_model()\n",
        "\n",
        "\n",
        "model = make_or_restore_model()\n",
        "callbacks = [\n",
        "    # This callback saves a SavedModel every 100 batches.\n",
        "    # We include the training loss in the saved model name.\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_dir + \"/ckpt-loss={loss:.2f}\", save_freq=100\n",
        "    )\n",
        "]\n",
        "model.fit(x_train, y_train, epochs=1, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da3ab58d5235"
      },
      "source": [
        "Você também pode escrever seu próprio callback para salvar e restaurar modelos.\n",
        "\n",
        "Para um guia completo sobre serialização e salvamento, consulte o [guia para salvar e serializar modelos](https://www.tensorflow.org/guide/keras/save_and_serialize/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9342cc2ddba"
      },
      "source": [
        "## Usando cronogramas de taxa de aprendizado\n",
        "\n",
        "Um padrão comum ao treinar modelos de aprendizado profundo é reduzir gradualmente o aprendizado à medida que o treinamento avança. Isso é geralmente conhecido como \"decaimento da taxa de aprendizado\".\n",
        "\n",
        "O cronograma de decaimento do aprendizado pode ser estático (fixado antecipadamente, em função da época atual ou do índice do lote atual) ou dinâmico (respondendo ao comportamento atual do modelo, em particular, a perda de validação).\n",
        "\n",
        "### Passando um cronograma para um otimizador\n",
        "\n",
        "Você pode usar um cronograma de decaimento de taxa de aprendizado estático facilmente, passando um objeto de cronograma como o argumento `learning_rate` no seu otimizador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "684f0ab6d3de"
      },
      "outputs": [],
      "source": [
        "initial_learning_rate = 0.1\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03b61ddd9586"
      },
      "source": [
        "Vários cronogramas estão disponíveis: `ExponentialDecay`, `PiecewiseConstantDecay`, `PolynomialDecay` e `InverseTimeDecay`.\n",
        "\n",
        "### Usando callbacks para implementar um cronograma de taxa de aprendizado dinâmico\n",
        "\n",
        "Um cronograma de taxa de aprendizado dinâmico (por exemplo, diminuindo a taxa de aprendizado quando a perda de validação não está mais melhorando) não pode ser alcançado com esses objetos de cronograma, pois o otimizador não tem acesso às métricas de validação.\n",
        "\n",
        "No entanto, os callbacks têm acesso a todas as métricas, incluindo métricas de validação! Assim, você pode obter esse padrão usando um callback que modifica a taxa de aprendizado atual no otimizador. Na verdade, isto faz parte da API como o callback `ReduceLROnPlateau`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f8b9539cd57"
      },
      "source": [
        "## Visualizando perda e métricas durante o treinamento\n",
        "\n",
        "A melhor maneira de ficar de olho no seu modelo durante o treinamento é usar o [TensorBoard](https://www.tensorflow.org/tensorboard), um aplicativo que roda no navegador, que pode ser executado localmente e que fornece:\n",
        "\n",
        "- Plotagens ao vivo da perda e métricas para treinamento e avaliação\n",
        "- (opcionalmente) Visualizações dos histogramas de suas ativações de camada\n",
        "- (opcionalmente) visualizações 3D dos espaços de incorporação aprendidos por suas camadas `Embedding`\n",
        "\n",
        "Se você instalou o TensorFlow com pip, poderá iniciar o TensorBoard através da linha de comando:\n",
        "\n",
        "```\n",
        "tensorboard --logdir=/full_path_to_your_logs\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2685d7ce531"
      },
      "source": [
        "### Usando o callback TensorBoard\n",
        "\n",
        "A maneira mais fácil de usar o TensorBoard com um modelo Keras e o método `fit()` é o callback `TensorBoard`.\n",
        "\n",
        "No caso mais simples, apenas especifique onde deseja que o callback grave os logs e pronto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f74247282ff6"
      },
      "outputs": [],
      "source": [
        "keras.callbacks.TensorBoard(\n",
        "    log_dir=\"/full_path_to_your_logs\",\n",
        "    histogram_freq=0,  # How often to log histogram visualizations\n",
        "    embeddings_freq=0,  # How often to log embedding visualizations\n",
        "    update_freq=\"epoch\",\n",
        ")  # How often to write logs (default: once per epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3614f8ba1e03"
      },
      "source": [
        "Para obter mais informações, consulte a [documentação do callback `TensorBoard`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/tensorboard/) ."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "train_and_evaluate.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
