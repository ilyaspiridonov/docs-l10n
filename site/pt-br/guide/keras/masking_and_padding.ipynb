{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bd329a4bbca"
      },
      "source": [
        "# Mascaramento e preenchimento com Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b208d0913b8"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/masking_and_padding\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Veja em TensorFlow.org</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/pt-br/guide/keras/masking_and_padding.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Executar no Google Colab</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/pt-br/guide/keras/masking_and_padding.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fonte em GitHub</a>\n",
        "</td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/pt-br/guide/keras/masking_and_padding.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Baixar notebook</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4ac441b1fc"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec52be14e686"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e94d7a46bda8"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "O **mascaramento** (masking) é uma maneira de dizer às camadas de processamento de sequência que determinados intervalos de tempo de uma entrada estão ausentes e, portanto, devem ser ignorados ao processar os dados.\n",
        "\n",
        "O **preenchimento** (padding) é uma forma especial de mascaramento em que as etapas mascaradas estão no início ou no final de uma sequência. O preenchimento vem da necessidade de codificar os dados da sequência em lotes contíguos: para que todas as sequências de um lote caibam em um determinado comprimento padrão, é necessário preencher ou truncar algumas sequências.\n",
        "\n",
        "Vamos dar uma olhada mais de perto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac6b121d6be0"
      },
      "source": [
        "## Preenchimento de dados de sequência\n",
        "\n",
        "Ao processar dados de sequência, é muito comum que amostras individuais tenham comprimentos diferentes. Considere o seguinte exemplo (texto tokenizado como palavras):\n",
        "\n",
        "```\n",
        "[\n",
        "  [\"Hello\", \"world\", \"!\"],\n",
        "  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
        "  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
        "]\n",
        "```\n",
        "\n",
        "Depois da pesquisa de vocabulário, os dados podem ser vetorizados como inteiros, por exemplo:\n",
        "\n",
        "```\n",
        "[\n",
        "  [71, 1331, 4231]\n",
        "  [73, 8, 3215, 55, 927],\n",
        "  [83, 91, 1, 645, 1253, 927],\n",
        "]\n",
        "```\n",
        "\n",
        "Os dados são uma lista aninhada em que amostras individuais têm comprimentos 3, 5 e 6, respectivamente. Como os dados de entrada para um modelo de aprendizado profundo devem ser um único tensor (de forma, por exemplo, `(batch_size, 6, vocab_size)` neste caso), as amostras que são mais curtas que o item mais longo precisam ser preenchidas com um valor especial para marcar posição (alternativamente, também pode-se truncar amostras longas antes de preencher amostras curtas).\n",
        "\n",
        "O Keras fornece uma função utilitária para truncar e preencher listas do Python para terem um comprimento comum: `tf.keras.preprocessing.sequence.pad_sequences` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb64fb185a05"
      },
      "outputs": [],
      "source": [
        "raw_inputs = [\n",
        "    [711, 632, 71],\n",
        "    [73, 8, 3215, 55, 927],\n",
        "    [83, 91, 1, 645, 1253, 927],\n",
        "]\n",
        "\n",
        "# By default, this will pad using 0s; it is configurable via the\n",
        "# \"value\" parameter.\n",
        "# Note that you could \"pre\" padding (at the beginning) or\n",
        "# \"post\" padding (at the end).\n",
        "# We recommend using \"post\" padding when working with RNN layers\n",
        "# (in order to be able to use the\n",
        "# CuDNN implementation of the layers).\n",
        "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    raw_inputs, padding=\"post\"\n",
        ")\n",
        "print(padded_inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03092b2da690"
      },
      "source": [
        "## Mascaramento\n",
        "\n",
        "Agora que todas as amostras têm um comprimento uniforme, o modelo precisa ser informado de que determinada parte dos dados é na verdade um preenchimento e que deve ser ignorada. Esse mecanismo é o **mascaramento** .\n",
        "\n",
        "Há três maneiras de introduzir máscaras de entrada nos modelos Keras:\n",
        "\n",
        "- Adicionando uma camada `keras.layers.Masking` .\n",
        "- Configurando uma camada `keras.layers.Embedding` com `mask_zero=True` .\n",
        "- Passando um argumento `mask` manualmente ao chamar camadas que suportam este argumento (por exemplo, camadas RNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6103601e5fff"
      },
      "source": [
        "## Camadas geradoras de máscaras: `Embedding` e `Masking`\n",
        "\n",
        "Nos bastidores, essas camadas criarão um tensor de máscara (tensor 2D com forma `(batch, sequence_length)`) que será anexado à saída do tensor retornada pela camada `Masking` ou `Embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2363b293483"
      },
      "outputs": [],
      "source": [
        "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
        "masked_output = embedding(padded_inputs)\n",
        "\n",
        "print(masked_output._keras_mask)\n",
        "\n",
        "masking_layer = layers.Masking()\n",
        "# Simulate the embedding lookup by expanding the 2D input to 3D,\n",
        "# with embedding dimension of 10.\n",
        "unmasked_embedding = tf.cast(\n",
        "    tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), tf.float32\n",
        ")\n",
        "\n",
        "masked_embedding = masking_layer(unmasked_embedding)\n",
        "print(masked_embedding._keras_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17e4bdb563b2"
      },
      "source": [
        "Como você pode ver no resultado impresso, a máscara é um tensor booleano 2D com formato `(batch_size, sequence_length)`, onde cada entrada `False` individual indica que o intervalo de tempo correspondente deve ser ignorado durante o processamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf11a0399fcf"
      },
      "source": [
        "## Propagação de máscaras na API Functional e na API Sequential\n",
        "\n",
        "Ao utilizar a API Functional ou a API Sequential, uma máscara gerada por uma camada `Embedding` ou `Masking` será propagada pela rede para qualquer camada que seja capaz de utilizá-las (por exemplo, camadas RNN). O Keras buscará automaticamente a máscara correspondente a uma entrada e a passará para qualquer camada que saiba como usá-la.\n",
        "\n",
        "Por exemplo, no modelo Sequential a seguir, a camada `LSTM` receberá automaticamente uma máscara, o que significa que ignorará os valores preenchidos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0adbecda288a"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential(\n",
        "    [layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True), layers.LSTM(32),]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ac6481a1d5"
      },
      "source": [
        "Este também é o caso do seguinte modelo da API Functional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f374ab06743d"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
        "outputs = layers.LSTM(32)(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f2c4b96ecb5"
      },
      "source": [
        "## Passando tensores de máscara diretamente para as camadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11dccb581014"
      },
      "source": [
        "As camadas que podem processar máscaras (como a camada `LSTM`) têm um argumento `mask` em seu método `__call__`.\n",
        "\n",
        "Enquanto isso, as camadas que produzem uma máscara (por exemplo, `Embedding`) expõem um método `compute_mask(input, previous_mask)` que você pode chamar.\n",
        "\n",
        "Assim, você pode passar a saída do método `compute_mask()` de uma camada produtora de máscaras para o método `__call__` de uma camada consumidora de máscaras, dessa forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1955aa63896b"
      },
      "outputs": [],
      "source": [
        "class MyLayer(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyLayer, self).__init__(**kwargs)\n",
        "        self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
        "        self.lstm = layers.LSTM(32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        # Note that you could also prepare a `mask` tensor manually.\n",
        "        # It only needs to be a boolean tensor\n",
        "        # with the right shape, i.e. (batch_size, timesteps).\n",
        "        mask = self.embedding.compute_mask(inputs)\n",
        "        output = self.lstm(x, mask=mask)  # The layer will ignore the masked values\n",
        "        return output\n",
        "\n",
        "\n",
        "layer = MyLayer()\n",
        "x = np.random.random((32, 10)) * 100\n",
        "x = x.astype(\"int32\")\n",
        "layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b04dd330f848"
      },
      "source": [
        "## Suporte ao mascaramento em suas camadas personalizadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8451a1a8ff27"
      },
      "source": [
        "Às vezes, pode ser necessário escrever camadas que geram uma máscara (como `Embedding`) ou camadas que precisem modificar a máscara atual.\n",
        "\n",
        "Por exemplo, qualquer camada que produza um tensor com uma dimensão de tempo diferente de sua entrada, tal como uma camada `Concatenate` que concatena na dimensão de tempo, precisará modificar a máscara atual para que as camadas seguintes possam levar considerar corretamente os intervalos de tempo mascarados.\n",
        "\n",
        "Para isto, sua camada deve implementar o método `layer.compute_mask()`, que produz uma nova máscara dada a entrada e a máscara atual.\n",
        "\n",
        "Aqui está um exemplo de uma camada `TemporalSplit` que precisa modificar a máscara atual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a06fb2194c0d"
      },
      "outputs": [],
      "source": [
        "class TemporalSplit(keras.layers.Layer):\n",
        "    \"\"\"Split the input tensor into 2 tensors along the time dimension.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Expect the input to be 3D and mask to be 2D, split the input tensor into 2\n",
        "        # subtensors along the time axis (axis 1).\n",
        "        return tf.split(inputs, 2, axis=1)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        # Also split the mask into 2 if it presents.\n",
        "        if mask is None:\n",
        "            return None\n",
        "        return tf.split(mask, 2, axis=1)\n",
        "\n",
        "\n",
        "first_half, second_half = TemporalSplit()(masked_embedding)\n",
        "print(first_half._keras_mask)\n",
        "print(second_half._keras_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "282b867dcd95"
      },
      "source": [
        "Aqui está outro exemplo de uma camada `CustomEmbedding` que é capaz de gerar uma máscara a partir de valores de entrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e760655cd39c"
      },
      "outputs": [],
      "source": [
        "class CustomEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n",
        "        super(CustomEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.mask_zero = mask_zero\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            dtype=\"float32\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        if not self.mask_zero:\n",
        "            return None\n",
        "        return tf.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "layer = CustomEmbedding(10, 32, mask_zero=True)\n",
        "x = np.random.random((3, 10)) * 9\n",
        "x = x.astype(\"int32\")\n",
        "\n",
        "y = layer(x)\n",
        "mask = layer.compute_mask(x)\n",
        "\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb34149eb837"
      },
      "source": [
        "## Aceitando a propagação de máscaras em camadas compatíveis\n",
        "\n",
        "A maioria das camadas não modifica a dimensão de tempo, então não há necessidade de se modificar a máscara atual. No entanto, elas ainda podem querer **propagar** a máscara atual, inalterada, para a camada seguinte. **Este é um comportamento opcional.** Por padrão, uma camada personalizada destruirá a máscara atual (já que o framework não tem como saber se a propagação da máscara é segura).\n",
        "\n",
        "Se você tiver uma camada personalizada que não modifica a dimensão de tempo e quiser que ela seja capaz de propagar a máscara de entrada atual, defina `self.supports_masking = True` no construtor da camada. Neste caso, o comportamento padrão de `compute_mask()` será de apenas passar adiante a máscara atual.\n",
        "\n",
        "Eis aqui um exemplo de uma camada configurada para permitir a propagação de máscaras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "895c35534d06"
      },
      "outputs": [],
      "source": [
        "class MyActivation(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyActivation, self).__init__(**kwargs)\n",
        "        # Signal that the layer is safe for mask propagation\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.relu(inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2e1e0a81995"
      },
      "source": [
        "Agora você pode usar essa camada personalizada entre uma camada geradora de máscaras (como `Embedding`) e uma camada consumidora de máscaras (como `LSTM` ), e ela passará a máscara para que ela alcance a camada consumidora de máscaras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "486e39e9a9a7"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
        "x = MyActivation()(x)  # Will pass the mask along\n",
        "print(\"Mask found:\", x._keras_mask)\n",
        "outputs = layers.LSTM(32)(x)  # Will receive the mask\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ab9c02f4ba"
      },
      "source": [
        "## Escrevendo camadas que precisam de informações de máscara\n",
        "\n",
        "Algumas camadas são *consumidoras* de máscara: elas aceitam um argumento `mask` na `call` e o usam para determinar se devem pular determinadas etapas de tempo.\n",
        "\n",
        "Para escrever uma camada assim, você pode simplesmente adicionar um argumento `mask=None` na sua assinatura `call`. A máscara associada às entradas será passada para sua camada sempre que estiver disponível.\n",
        "\n",
        "Veja a seguir um exemplo simples: uma camada que calcula um softmax sobre a dimensão de tempo (eixo 1) de uma sequência de entrada, enquanto descarta intervalos de tempo mascarados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9774bcd59908"
      },
      "outputs": [],
      "source": [
        "class TemporalSoftmax(keras.layers.Layer):\n",
        "    def call(self, inputs, mask=None):\n",
        "        broadcast_float_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
        "        inputs_exp = tf.exp(inputs) * broadcast_float_mask\n",
        "        inputs_sum = tf.reduce_sum(\n",
        "            inputs_exp * broadcast_float_mask, axis=-1, keepdims=True\n",
        "        )\n",
        "        return inputs_exp / inputs_sum\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs)\n",
        "x = layers.Dense(1)(x)\n",
        "outputs = TemporalSoftmax()(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "y = model(np.random.randint(0, 10, size=(32, 100)), np.random.random((32, 100, 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6373f43bbe18"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Isso é tudo o que você precisa saber sobre preenchimento e mascaramento no Keras. Para recapitular:\n",
        "\n",
        "- \"Mascaramento\" é como as camadas são capazes de saber quando pular/ignorar determinados intervalos de tempo nas entradas de sequências.\n",
        "- Algumas camadas são geradoras de máscaras: `Embedding` pode gerar uma máscara a partir de valores de entrada (se `mask_zero=True` ), assim como a camada `Masking`.\n",
        "- Algumas camadas são consumidoras de máscaras: elas expõem um argumento `mask` em seu método `__call__`. É o caso das camadas RNN.\n",
        "- Nas APIs Functional e Sequential, as informações de máscara são propagadas automaticamente.\n",
        "- Ao usar camadas de forma standalone, você pode passar os argumentos `mask` para as camadas manualmente.\n",
        "- Você pode facilmente escrever camadas que modificam a máscara atual, que geram uma nova máscara ou que consomem a máscara associada às entradas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "masking_and_padding.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
